{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "inner-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "# import tensorflow_io as tfio\n",
    "import random\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "invalid-association",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            filename  length  chroma_stft_mean  chroma_stft_var  rms_mean  \\\n",
      "0    blues.00000.wav  661794          0.350088         0.088757  0.130228   \n",
      "1    blues.00001.wav  661794          0.340914         0.094980  0.095948   \n",
      "2    blues.00002.wav  661794          0.363637         0.085275  0.175570   \n",
      "3    blues.00003.wav  661794          0.404785         0.093999  0.141093   \n",
      "4    blues.00004.wav  661794          0.308526         0.087841  0.091529   \n",
      "..               ...     ...               ...              ...       ...   \n",
      "995   rock.00095.wav  661794          0.352063         0.080487  0.079486   \n",
      "996   rock.00096.wav  661794          0.398687         0.075086  0.076458   \n",
      "997   rock.00097.wav  661794          0.432142         0.075268  0.081651   \n",
      "998   rock.00098.wav  661794          0.362485         0.091506  0.083860   \n",
      "999   rock.00099.wav  661794          0.358401         0.085884  0.054454   \n",
      "\n",
      "      rms_var  spectral_centroid_mean  spectral_centroid_var  \\\n",
      "0    0.002827             1784.165850          129774.064525   \n",
      "1    0.002373             1530.176679          375850.073649   \n",
      "2    0.002746             1552.811865          156467.643368   \n",
      "3    0.006346             1070.106615          184355.942417   \n",
      "4    0.002303             1835.004266          343399.939274   \n",
      "..        ...                     ...                    ...   \n",
      "995  0.000345             2008.149458          282174.689224   \n",
      "996  0.000588             2006.843354          182114.709510   \n",
      "997  0.000322             2077.526598          231657.968040   \n",
      "998  0.001211             1398.699344          240318.731073   \n",
      "999  0.000336             1609.795082          422203.216152   \n",
      "\n",
      "     spectral_bandwidth_mean  spectral_bandwidth_var  ...  mfcc16_var  \\\n",
      "0                2002.449060            85882.761315  ...   52.420910   \n",
      "1                2039.036516           213843.755497  ...   55.356403   \n",
      "2                1747.702312            76254.192257  ...   40.598766   \n",
      "3                1596.412872           166441.494769  ...   44.427753   \n",
      "4                1748.172116            88445.209036  ...   86.099236   \n",
      "..                       ...                     ...  ...         ...   \n",
      "995              2106.541053            88609.749506  ...   45.050526   \n",
      "996              2068.942009            82426.016726  ...   33.851742   \n",
      "997              1927.293153            74717.124394  ...   33.597008   \n",
      "998              1818.450280           109090.207161  ...   46.324894   \n",
      "999              1797.213044           120115.632927  ...   59.167755   \n",
      "\n",
      "     mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  \\\n",
      "0      -1.690215   36.524071    -0.408979   41.597103    -2.303523   \n",
      "1      -0.731125   60.314529     0.295073   48.120598    -0.283518   \n",
      "2      -7.729093   47.639427    -1.816407   52.382141    -3.439720   \n",
      "3      -3.319597   50.206673     0.636965   37.319130    -0.619121   \n",
      "4      -5.454034   75.269707    -0.916874   53.613918    -4.404827   \n",
      "..           ...         ...          ...         ...          ...   \n",
      "995   -13.289984   41.754955     2.484145   36.778877    -6.713265   \n",
      "996   -10.848309   39.395096     1.881229   32.010040    -7.461491   \n",
      "997   -12.845291   36.367264     3.440978   36.001110   -12.588070   \n",
      "998    -4.416050   43.583942     1.556207   34.331261    -5.041897   \n",
      "999    -7.069775   73.760391     0.028346   76.504326    -2.025783   \n",
      "\n",
      "     mfcc19_var  mfcc20_mean  mfcc20_var  label  \n",
      "0     55.062923     1.221291   46.936035  blues  \n",
      "1     51.106190     0.531217   45.786282  blues  \n",
      "2     46.639660    -2.231258   30.573025  blues  \n",
      "3     37.259739    -3.407448   31.949339  blues  \n",
      "4     62.910812   -11.703234   55.195160  blues  \n",
      "..          ...          ...         ...    ...  \n",
      "995   54.866825    -1.193787   49.950665   rock  \n",
      "996   39.196327    -2.795338   31.773624   rock  \n",
      "997   42.502201    -2.106337   29.865515   rock  \n",
      "998   47.227180    -3.590644   41.299088   rock  \n",
      "999   72.189316     1.155239   49.662510   rock  \n",
      "\n",
      "[1000 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "# get minimal length of music sample\n",
    "ds_info = pd.read_csv('data/gtzan/features_30_sec.csv')\n",
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-sequence",
   "metadata": {},
   "source": [
    "# GTZAN Dataset - Music Genre Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "finite-belly",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/features_30_sec.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ec85036e6564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get file paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mds_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/features_30_sec.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/tf-gpu/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/tf-gpu/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/tf-gpu/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/tf-gpu/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             )\n\u001b[1;32m   1044\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/tf-gpu/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1863\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/tf-gpu/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \"\"\"\n\u001b[0;32m-> 1357\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1358\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/tf-gpu/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    640\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/features_30_sec.csv'"
     ]
    }
   ],
   "source": [
    "# get file paths\n",
    "ds_info = pd.read_csv('data/features_30_sec.csv')\n",
    "\n",
    "file_names = ds_info['filename'].values\n",
    "file_names.astype(np.str)\n",
    "file_paths = np.array(['data/no_genres/' + file for file in file_names])\n",
    "\n",
    "file_path_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "\n",
    "# read files and decode them\n",
    "ds = [tfio.audio.AudioIOTensor(path) for path in file_paths]\n",
    "\n",
    "# make length for each sample the same\n",
    "min_len = min([a.to_tensor().shape[0] for a in ds])\n",
    "\n",
    "# extract np arrays from the tf.ds\n",
    "ds_np = np.array([a.to_tensor().numpy()[0:min_len] for a in ds])\n",
    "\n",
    "ds_rate = np.array([a.rate.numpy() for a in ds])\n",
    "if np.all(ds_rate == ds_rate[0]):\n",
    "    ds_rate = ds_rate[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "radical-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 20\n",
    "k = 10\n",
    "\n",
    "N = 10\n",
    "d = 1\n",
    "filenames = file_paths.tolist()\n",
    "\n",
    "\n",
    "# load audio\n",
    "def decode_audio(audio_binary):\n",
    "    audio, sample_rate = tf.audio.decode_wav(audio_binary)\n",
    "    return tf.squeeze(audio, axis=-1), sample_rate\n",
    "\n",
    "def music_generator():\n",
    "    \"\"\"\n",
    "    assumes we have a list of all filenames/paths\n",
    "\n",
    "    T: number of time-steps to use for the context embedding c_t\n",
    "\n",
    "    k: number of future time-steps to predict\n",
    "\n",
    "    N: number of samples (1 positive + N-1 negative samples)\n",
    "\n",
    "    d: window size in seconds\n",
    "\n",
    "    filenames: list of all filepaths to all audio files\n",
    "    \"\"\"\n",
    "    global filenames, T, k, N, d\n",
    "    while True:\n",
    "        \n",
    "        #randomly select positive sample filenames from list        \n",
    "        samples = random.sample(filenames, N)\n",
    "        positive_sample = samples[0]\n",
    "        negative_samples = samples[1:]\n",
    "\n",
    "        # positive sample\n",
    "        positive_audio = tf.io.read_file(positive_sample)\n",
    "        positive_audio, sample_rate = decode_audio(positive_audio)\n",
    "        \n",
    "        end = sample_rate * (T+k)   \n",
    "        window_size = int(sample_rate * d)\n",
    "        positive_audio = tf.reshape(positive_audio[:end], (T+k, window_size, 1))\n",
    "\n",
    "        # negative samples (find a way to do it without a for loop pls)\n",
    "        sample_tensors = []\n",
    "        sample_tensors.append(positive_audio)\n",
    "\n",
    "        for ns in negative_samples:\n",
    "            ns = tf.io.read_file(ns)\n",
    "            ns, sample_rate = decode_audio(ns)\n",
    "            window_size = sample_rate * d\n",
    "            end = sample_rate * (T+k)   \n",
    "\n",
    "            ns = tf.reshape(ns[:end], (T+k,window_size,1))\n",
    "\n",
    "            # only take the last k entries (pls double check this)\n",
    "            ns = ns[T:T+k]\n",
    "\n",
    "            sample_tensors.append(ns)\n",
    "\n",
    "        # concatenate all tensors, making its shape (T+k*N, window_size,1)\n",
    "        data = tf.concat(sample_tensors, axis= 0)\n",
    "\n",
    "        yield data                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "prescription-puppy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(120, 22050, 1), dtype=float32, numpy=\n",
       "array([[[-0.23886108],\n",
       "        [-0.41534424],\n",
       "        [-0.3161621 ],\n",
       "        ...,\n",
       "        [ 0.02081299],\n",
       "        [ 0.01882935],\n",
       "        [ 0.01901245]],\n",
       "\n",
       "       [[ 0.02401733],\n",
       "        [ 0.02923584],\n",
       "        [ 0.03115845],\n",
       "        ...,\n",
       "        [ 0.05682373],\n",
       "        [ 0.06643677],\n",
       "        [ 0.06881714]],\n",
       "\n",
       "       [[ 0.06442261],\n",
       "        [ 0.05459595],\n",
       "        [ 0.04019165],\n",
       "        ...,\n",
       "        [ 0.3184204 ],\n",
       "        [ 0.24853516],\n",
       "        [ 0.2013855 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.21835327],\n",
       "        [-0.15158081],\n",
       "        [-0.05319214],\n",
       "        ...,\n",
       "        [-0.15216064],\n",
       "        [-0.13912964],\n",
       "        [-0.12341309]],\n",
       "\n",
       "       [[-0.11651611],\n",
       "        [-0.12738037],\n",
       "        [-0.14666748],\n",
       "        ...,\n",
       "        [ 0.04693604],\n",
       "        [ 0.03289795],\n",
       "        [ 0.01449585]],\n",
       "\n",
       "       [[ 0.03625488],\n",
       "        [ 0.03643799],\n",
       "        [-0.03161621],\n",
       "        ...,\n",
       "        [ 0.0255127 ],\n",
       "        [ 0.01348877],\n",
       "        [-0.06192017]]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = music_generator()\n",
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-ratio",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch, t+k, rate, 1)\n",
    "BATCH_SIZE = 64\n",
    "SAMPLE_LEN = 30\n",
    "RATE = ds_rate\n",
    "DS_LEN = len(ds_np)\n",
    "T = 20\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def music_generator():\n",
    "    global ds_np, RATE, SAMPLE_LEN, BATCH_SIZE, T, K, DS_LEN\n",
    "    if not ds_np[0].shape[0]/RATE == SAMPLE_LEN:\n",
    "        end = RATE*SAMPLE_LEN\n",
    "        ds_np = np.array([a.to_tensor().numpy()[0:end] for a in ds])\n",
    "    \n",
    "    while True:\n",
    "        idx = random.randint(0, DS_LEN)\n",
    "        sample = ds_np[idx].reshape((-1, RATE))\n",
    "        #sample = np.expand_dims(sample, axis=0)\n",
    "        \n",
    "        yield sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = music_generator()\n",
    "next(gen).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_generator(generator=music_generator,\n",
    "                                    output_types=tf.float32,\n",
    "                                    # (time_len, 30), (time_len,2)\n",
    "                                    output_shapes=(SAMPLE_LEN, RATE)\n",
    "                                    )\n",
    "ds = ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ds.take(1):\n",
    "    print(i.shape)\n",
    "\n",
    "# in sec\n",
    "\n",
    "delta_t = 30\n",
    "time_timestep = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 512  # latent dim z_t\n",
    "c_dim = 256  # dim of g_ar output c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoNCE (tf.keras.losses.Loss):\n",
    "    '''\n",
    "    Compute loss given batch times f matrices with dim (K x N)\n",
    "    '''\n",
    "\n",
    "    def __call__(self, f):\n",
    "        # input dim: [batch, K, N]\n",
    "        denominator = tf.reduce_sum(f, axis=2)  # [batch, K]\n",
    "        losses = - tf.math.log(f[:,:,0] / denominator)  # first column is positive\n",
    "        return tf.reduce_mean(losses, axis=1)  # [batch]. Take a mean over k timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder (tf.keras.layers.Layer):\n",
    "    '''\n",
    "    g_enc: strided 1d convolution\n",
    "    '''\n",
    "\n",
    "    def __init__ (self, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        s = [5,4,2,2,2]  # stride sizes\n",
    "        k = [10,8,4,4,4]  # kernel sizes\n",
    "        f = [512,512,512,512,512]  # num filters\n",
    "\n",
    "        # input dim: [batch, T+K*N, d, 1]\n",
    "        self.layers = []\n",
    "        for l in range(5):\n",
    "            self.layers.append(tf.keras.layers.Conv1D(f[l],k[l],s[l]))\n",
    "            self.layers.append(tf.keras.layers.BatchNormalization())\n",
    "            self.layers.append(tf.keras.layers.LeakyReLU())\n",
    "        self.layers.append(tf.keras.layers.GlobalAveragePooling1D())\n",
    "        self.layers.append(tf.keras.layers.Dense(z_dim, activation='tanh'))\n",
    "        # ouput dim:[batch, T+K*N, z]\n",
    "\n",
    "    def call (self, x, training):\n",
    "        \n",
    "        for l in self.layers:\n",
    "            try:  # batch normalization \n",
    "                x = l(x, training)\n",
    "            except:\n",
    "                x = l(x)\n",
    "        return x  \n",
    "\n",
    "\n",
    "class Autoregressive (tf.keras.layers.Layer):\n",
    "    '''\n",
    "    g_ar: GRU RNN\n",
    "    '''\n",
    "\n",
    "    def __init__ (self, c_dim):\n",
    "        super(Autoregressive, self).__init__()\n",
    "        # input dim: [batch, T, z]\n",
    "        self.l = tf.keras.layers.GRU(c_dim, name='ar_context') \n",
    "        # output dim:[batch, c] since return_seq is False\n",
    "\n",
    "    def call (self, z):\n",
    "        return self.l(z) \n",
    "\n",
    "\n",
    "class Predict_z (tf.keras.layers.Layer):\n",
    "    '''\n",
    "    transformation of c_t, currently linear (W_k) for all future timesteps\n",
    "    '''\n",
    "\n",
    "    def __init__ (self, z_dim, K):\n",
    "        super(Predict_z, self).__init__()\n",
    "        \n",
    "        # input_dim: [batch, c]\n",
    "        self.layers = []\n",
    "        for k in range(K):  # k different layers for each timestep\n",
    "            self.layers.append(tf.keras.layers.Dense(z_dim)) \n",
    "\n",
    "    def call(self, c_t):\n",
    "        # TODO: maybe size should be multidimensional\n",
    "        z_pred = tf.TensorArray(tf.float32, size=len(self.layers))\n",
    "        for l in tf.range(len(self.layers)):  \n",
    "            z_pred = z_pred.write(l, self.layers[l](c_t))  # apply for each k\n",
    "            z_pred_t = z_pred.stack()\n",
    "            # [K, batch, z]\n",
    "        return tf.transpose(z_pred_t, perm=[1,0,2])  # output_dim: [batch, K, z]\n",
    "\n",
    "\n",
    "def compute_f (z, z_pred):\n",
    "    '''\n",
    "    compute f following eq(3) in the paper to be batch (K x N) matrices.\n",
    "    First column is the postive sample.\n",
    "    '''\n",
    "\n",
    "    # z input dim: [batch, K, N, z], \n",
    "    z = tf.expand_dims(z, axis=-2)  # [batch, K, N, 1, z]\n",
    "    \n",
    "    # z_pred input dim: [batch, K, z]\n",
    "    pred = tf.repeat(z_pred, repeats=z.shape[2], axis=-2)  # [batch, K*N, z]\n",
    "    pred = tf.reshape(pred, shape=[z.shape[0],z.shape[1],z.shape[2],z.shape[-1]])  # [batch, K, N, z]\n",
    "    pred = tf.expand_dims(pred, axis=-1)  # [batch, K, N, z, 1]\n",
    "\n",
    "    dot_prod = tf.linalg.matmul(z, pred)  # [batch, K, N, 1, 1]\n",
    "    dot_prod = tf.squeeze(dot_prod, axis=[-2,-1])  # [batch, K, N]\n",
    "    dot_prod = tf.exp(dot_prod)\n",
    "    return dot_prod  # output dim: [batch, K, N]\n",
    "\n",
    "\n",
    "class CPC (tf.keras.models.Model):\n",
    "    '''\n",
    "    put everything together. Return f_k for every k\n",
    "    '''\n",
    "\n",
    "    def __init__ (self, num_time_observations, num_time_future, num_negative_samples, z_dim, c_dim):\n",
    "        super(CPC, self).__init__()\n",
    "        self.T = num_time_observations\n",
    "        self.K = num_time_future\n",
    "        self.N = num_negative_samples\n",
    "        self.z = z_dim\n",
    "        self.c = c_dim\n",
    "\n",
    "        self.g_enc = Encoder(self.z)\n",
    "        self.g_ar = Autoregressive(self.c)\n",
    "        self.p_z = Predict_z(z_dim=self.z, K=self.K)\n",
    "\n",
    "    def call(self, x, training=False):  \n",
    "        # input dim: [batch, T+K*N, d, 1]\n",
    "        print('input dim: ', x.shape)\n",
    "        # Embedding\n",
    "        z_t = tf.keras.layers.TimeDistributed( # dim 1 is the temporal dim \n",
    "            self.g_enc)(x, training=training)  # [batch, T+K*N, z]\n",
    "        print('embedding dim: ', z_t.shape)    \n",
    "        \n",
    "\n",
    "        # Split current observation embeddings and future embeddings\n",
    "        z_obs = z_t[:, :self.T]  # t = {0,...,T}, dim: [batch, T, z]\n",
    "        z_future = z_t[:, self.T:]  # t = {T+1,,,T+K} for N samples, dim:[batch, K*N, z]\n",
    "        z_future = tf.reshape(z_future, [-1, self.K, self.N, self.z])  # [batch, K, N, z]\n",
    "        print('embedding obs:', z_obs.shape)\n",
    "        print('embedding pred:', z_future.shape)\n",
    "\n",
    "        # Predict embeddings\n",
    "        c_T = self.g_ar(z_obs)  # [batch, c]\n",
    "        print('context:', c_T.shape)\n",
    "        z_pred = self.p_z(c_T)  # [batch, K, z]\n",
    "        print('transformed_context:', z_pred.shape)\n",
    "\n",
    "        # Compute f matrices\n",
    "        f_mat = compute_f(z_future, z_pred)  # [batch, K, N]\n",
    "\n",
    "        return f_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 1\n",
    "T = 8\n",
    "K = 3\n",
    "N = 5\n",
    "d = 1000\n",
    "\n",
    "data = np.random.rand(batch, T+K*N, d, 1)\n",
    "data = tf.constant(data)\n",
    "print('input shape:', data.shape)\n",
    "\n",
    "cpc = CPC(T, K, N, z_dim, c_dim)\n",
    "f_mat = cpc(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

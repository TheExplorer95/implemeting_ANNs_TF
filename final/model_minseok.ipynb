{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQTycFXV8dkVJKxK1R06S/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spinkk/Implementing-ANNs-with-Tensorflow/blob/main/final/model_minseok.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqOowDStD4oS"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eDeeoKKFsrx"
      },
      "source": [
        "# Hyperparam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TicBo2lFsaf"
      },
      "source": [
        "z_dim = 512  # latent dim z_t\n",
        "c_dim = 256  # dim of g_ar output c_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRuwXidaKXwf"
      },
      "source": [
        "# Model\n",
        "- Encoder: convolutional\n",
        "- Autoregressive: GRU\n",
        "- transformation of context: linear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K1bAT6zEBwo"
      },
      "source": [
        "class Encoder (tf.keras.layers.Layer):\n",
        "    '''\n",
        "    g_enc: strided 1d convolution\n",
        "    '''\n",
        "\n",
        "    def __init__ (self, z_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        s = [5,4,2,2,2]  # stride sizes\n",
        "        k = [10,8,4,4,4]  # kernel sizes\n",
        "        f = [512,512,512,512,512]  # num filters\n",
        "\n",
        "        # input dim: [batch, T+K*N, d, 1]\n",
        "        self.layers = []\n",
        "        for l in range(5):\n",
        "            self.layers.append(tf.keras.layers.Conv1D(f[l],k[l],s[l]))\n",
        "            self.layers.append(tf.keras.layers.BatchNormalization())\n",
        "            self.layers.append(tf.keras.layers.LeakyReLU())\n",
        "        self.layers.append(tf.keras.layers.GlobalAveragePooling1D())\n",
        "        self.layers.append(tf.keras.layers.Dense(z_dim, activation='tanh'))\n",
        "        # ouput dim:[batch, T+K*N, z]\n",
        "\n",
        "    def call (self, x, training):\n",
        "        \n",
        "        for l in self.layers:\n",
        "            try:  # batch normalization \n",
        "                x = l(x, training)\n",
        "            except:\n",
        "                x = l(x)\n",
        "        return x  \n",
        "\n",
        "\n",
        "class Autoregressive (tf.keras.layers.Layer):\n",
        "    '''\n",
        "    g_ar: GRU RNN\n",
        "    '''\n",
        "\n",
        "    def __init__ (self, c_dim):\n",
        "        super(Autoregressive, self).__init__()\n",
        "        # input dim: [batch, T, z]\n",
        "        self.l = tf.keras.layers.GRU(c_dim, name='ar_context') \n",
        "        # output dim:[batch, c] since return_seq is False\n",
        "\n",
        "    def call (self, z):\n",
        "        return self.l(z) \n",
        "\n",
        "\n",
        "class Predict_z (tf.keras.layers.Layer):\n",
        "    '''\n",
        "    transformation of c_t, currently linear (W_k) for all future timesteps\n",
        "    '''\n",
        "\n",
        "    def __init__ (self, z_dim, K):\n",
        "        super(Predict_z, self).__init__()\n",
        "        \n",
        "        # input_dim: [batch, c]\n",
        "        self.layers = []\n",
        "        for k in range(K):  # k different layers for each timestep\n",
        "            self.layers.append(tf.keras.layers.Dense(z_dim)) \n",
        "\n",
        "    def call(self, c_t):\n",
        "        # TODO: maybe size should be multidimensional\n",
        "        z_pred = tf.TensorArray(tf.float32, size=len(self.layers))\n",
        "        for l in tf.range(len(self.layers)):  \n",
        "            z_pred = z_pred.write(l, self.layers[l](c_t))  # apply for each k\n",
        "            z_pred_t = z_pred.stack()\n",
        "            # [K, batch, z]\n",
        "        return tf.transpose(z_pred_t, perm=[1,0,2])  # output_dim: [batch, K, z]\n",
        "\n",
        "\n",
        "def compute_f (z, z_pred):\n",
        "    '''\n",
        "    compute f following eq(3) in the paper to be batch (K x N) matrices.\n",
        "    First column is the postive sample.\n",
        "    '''\n",
        "\n",
        "    # z input dim: [batch, K, N, z], \n",
        "    z = tf.expand_dims(z, axis=-2)  # [batch, K, N, 1, z]\n",
        "    \n",
        "    # z_pred input dim: [batch, K, z]\n",
        "    pred = tf.repeat(z_pred, repeats=z.shape[2], axis=-2)  # [batch, K*N, z]\n",
        "    pred = tf.reshape(pred, shape=[z.shape[0],z.shape[1],z.shape[2],z.shape[-1]])  # [batch, K, N, z]\n",
        "    pred = tf.expand_dims(pred, axis=-1)  # [batch, K, N, z, 1]\n",
        "\n",
        "    dot_prod = tf.linalg.matmul(z, pred)  # [batch, K, N, 1, 1]\n",
        "    dot_prod = tf.squeeze(dot_prod, axis=[-2,-1])  # [batch, K, N]\n",
        "    dot_prod = tf.exp(dot_prod)\n",
        "    return dot_prod  # output dim: [batch, K, N]\n",
        "\n",
        "\n",
        "class CPC (tf.keras.models.Model):\n",
        "    '''\n",
        "    put everything together. Return f_k for every k\n",
        "    '''\n",
        "\n",
        "    def __init__ (self, num_time_observations, num_time_future, num_negative_samples, z_dim, c_dim):\n",
        "        super(CPC, self).__init__()\n",
        "        self.T = num_time_observations\n",
        "        self.K = num_time_future\n",
        "        self.N = num_negative_samples\n",
        "        self.z = z_dim\n",
        "        self.c = c_dim\n",
        "\n",
        "        self.g_enc = Encoder(self.z)\n",
        "        self.g_ar = Autoregressive(self.c)\n",
        "        self.p_z = Predict_z(z_dim=self.z, K=self.K)\n",
        "\n",
        "    def call(self, x, training=False):  \n",
        "        # input dim: [batch, T+K*N, d, 1]\n",
        "        print('input dim: ', x.shape)\n",
        "        # Embedding\n",
        "        z_t = tf.keras.layers.TimeDistributed( # dim 1 is the temporal dim \n",
        "            self.g_enc)(x, training=training)  # [batch, T+K*N, z]\n",
        "        print('embedding dim: ', z_t.shape)    \n",
        "        \n",
        "\n",
        "        # Split current observation embeddings and future embeddings\n",
        "        z_obs = z_t[:, :self.T]  # t = {0,...,T}, dim: [batch, T, z]\n",
        "        z_future = z_t[:, self.T:]  # t = {T+1,,,T+K} for N samples, dim:[batch, K*N, z]\n",
        "        z_future = tf.reshape(z_future, [-1, self.K, self.N, self.z])  # [batch, K, N, z]\n",
        "        print('embedding obs:', z_obs.shape)\n",
        "        print('embedding pred:', z_future.shape)\n",
        "\n",
        "        # Predict embeddings\n",
        "        c_T = self.g_ar(z_obs)  # [batch, c]\n",
        "        print('context:', c_T.shape)\n",
        "        z_pred = self.p_z(c_T)  # [batch, K, z]\n",
        "        print('transformed_context:', z_pred.shape)\n",
        "\n",
        "        # Compute f matrices\n",
        "        f_mat = compute_f(z_future, z_pred)  # [batch, K, N]\n",
        "\n",
        "        return f_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8DUcP0ZWL4O",
        "outputId": "54e1a9e8-2337-40b8-f40a-6cd6b2f1d3e8"
      },
      "source": [
        "batch = 1\n",
        "T = 8\n",
        "K = 3\n",
        "N = 5\n",
        "d = 1000\n",
        "\n",
        "data = np.random.rand(batch, T+K*N, d, 1)\n",
        "data = tf.constant(data)\n",
        "print('input shape:', data.shape)\n",
        "\n",
        "cpc = CPC(T, K, N, z_dim, c_dim)\n",
        "f_mat = cpc(data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape: (1, 23, 1000, 1)\n",
            "input dim:  (1, 23, 1000, 1)\n",
            "embedding dim:  (1, 23, 512)\n",
            "embedding obs: (1, 8, 512)\n",
            "embedding pred: (1, 3, 5, 512)\n",
            "context: (1, 256)\n",
            "transformed_context: (1, 3, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga0L03pPelg-"
      },
      "source": [
        "class InfoNCE (tf.keras.losses.Loss):\n",
        "    '''\n",
        "    Compute loss given batch times f matrices with dim (K x N)\n",
        "    '''\n",
        "\n",
        "    def __call__(self, f):\n",
        "        # input dim: [batch, K, N]\n",
        "        denominator = tf.reduce_sum(f, axis=2)  # [batch, K]\n",
        "        losses = - tf.math.log(f[:,:,0] / denominator)  # first column is positive\n",
        "        return tf.reduce_mean(losses, axis=1)  # [batch]. Take a mean over k timesteps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvHQlbBWR26f",
        "outputId": "10a76aa9-4220-40fe-ad18-3c6d44db40d0"
      },
      "source": [
        "losses = InfoNCE()\n",
        "losses(f_mat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.6093172], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    }
  ]
}
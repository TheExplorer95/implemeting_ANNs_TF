{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CPC Modules.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "408dbc681281422ab3cf95cb8e15079e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c0b7d277bd124b0683b0e5f52ed791a2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d9b002eb13f54326b0f8d2a0a9b4d36a",
              "IPY_MODEL_1f37a9612d5d481bab9fafc8fed89570"
            ]
          }
        },
        "c0b7d277bd124b0683b0e5f52ed791a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9b002eb13f54326b0f8d2a0a9b4d36a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a237664718a245558453325759cba9ed",
            "_dom_classes": [],
            "description": "Dl Completed...:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd1245c0383245309e6ecf5d2177760d"
          }
        },
        "1f37a9612d5d481bab9fafc8fed89570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7c64bd66dbea4c2eb5637d36c94bcb93",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/1 [15:11&lt;?, ? url/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2aa831fdddb54c91b9be74175cbc664e"
          }
        },
        "a237664718a245558453325759cba9ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd1245c0383245309e6ecf5d2177760d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7c64bd66dbea4c2eb5637d36c94bcb93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2aa831fdddb54c91b9be74175cbc664e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb6e8e0ea67444238f796a1977426ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_76d91dac564c4deab5c8278105844107",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_df0fbfdb0d7f428abd5f5e83a5308b31",
              "IPY_MODEL_8778a44bf13e48099cd74db6cab75380"
            ]
          }
        },
        "76d91dac564c4deab5c8278105844107": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df0fbfdb0d7f428abd5f5e83a5308b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_68585baed1384c729dc9657892bf5931",
            "_dom_classes": [],
            "description": "Dl Size...:  66%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77c222efbaf54e0194834cf2714afb8f"
          }
        },
        "8778a44bf13e48099cd74db6cab75380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b99f81b07134496f95a2c531c3f1259d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 774/1168 [15:11&lt;07:43,  1.18s/ MiB]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_908db350dbc6475bb4f4d105ed448c99"
          }
        },
        "68585baed1384c729dc9657892bf5931": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77c222efbaf54e0194834cf2714afb8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b99f81b07134496f95a2c531c3f1259d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "908db350dbc6475bb4f4d105ed448c99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c778fb61a444729ad05950f844f85b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_783eb0f618e149568bc2f78c79cbaa4d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a3db588eaa5745c09c3ec6bd5191bc7f",
              "IPY_MODEL_cd8fd2d21e584d86acb2b50a16c7fc8b"
            ]
          }
        },
        "783eb0f618e149568bc2f78c79cbaa4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a3db588eaa5745c09c3ec6bd5191bc7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f9c3e6abfdfd44f597c655bae5de07c1",
            "_dom_classes": [],
            "description": "Extraction completed...: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61aa1736775c473f8cc4d24f413059f9"
          }
        },
        "cd8fd2d21e584d86acb2b50a16c7fc8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6eda82f853b04d25b5bb16081dd6a925",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/0 [15:11&lt;?, ? file/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5106a82d067f4839be0f57f09c534d73"
          }
        },
        "f9c3e6abfdfd44f597c655bae5de07c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61aa1736775c473f8cc4d24f413059f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6eda82f853b04d25b5bb16081dd6a925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5106a82d067f4839be0f57f09c534d73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxCLAomZiOf7",
        "outputId": "2524ef9a-ec75-426d-9c83-d7faf7e5bf0f"
      },
      "source": [
        "!pip install tensorflow_io\n",
        "!pip install pydub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_io in /usr/local/lib/python3.7/dist-packages (0.17.0)\n",
            "Requirement already satisfied: tensorflow<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.4.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.36.2)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.10.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.12.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.6.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.12.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.7.4.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.15.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.4.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.32.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.19.5)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.12)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.12.4)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (54.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.4.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.28.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2020.12.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (4.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.4.8)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "960cjYFmgyim"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "%config Completer.use_jedi = False\n",
        "# import globals\n",
        "# import preprocess_data\n",
        "# from contrPredCod_model import CPC, InfoNCE\n",
        "# from classifier_model import Classifier\n",
        "# from training import main_train_eval_loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4BgNxw1lqhW"
      },
      "source": [
        "# globals.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpz-zwWblr4y"
      },
      "source": [
        "def initialize():\n",
        "    global data_generator_arguments\n",
        "    data_generator_arguments = {}\n",
        "\n",
        "    global encoder_args\n",
        "    encoder_args = {}\n",
        "    \n",
        "    global ar_args\n",
        "    ar_args = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6Q4Du8eoofQ"
      },
      "source": [
        "# hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFyfX_SwgkGJ"
      },
      "source": [
        "### Hyperparameters ###\n",
        "# data path\n",
        "cwd = '/content/drive/MyDrive/Colab Notebooks/WS2021/ANN/final'\n",
        "cpc_data_path = cwd+\"/data/2000songs.zip (Unzipped Files)/2000songs\"  # cpc train data\n",
        "files = os.listdir(cpc_data_path)\n",
        "filepaths = [os.path.join(cpc_data_path, f) for f in files]\n",
        "gtzan_feature_path = cwd+'/data/gtzan/features_30_sec.csv'  # baseline features\n",
        "weight_path = cwd+'/model/cpc'  # where to save weights\n",
        "load_path = False  # from where to load weights\n",
        "\n",
        "# CPC data params\n",
        "# TODO: globals.init if using python scipts and importing\n",
        "initialize()  # init the global variable\n",
        "data_generator_arguments = {\n",
        "    \"T\": 27,  # timestep\n",
        "    \"k\": 3,  # timestep\n",
        "    \"N\": 8,  # number\n",
        "    \"full_duration\": 4,  # sec\n",
        "    \"original_sr\": 22050,  # Hz\n",
        "    \"desired_sr\": 4410,  # Hz\n",
        "    \"filepaths\": filepaths\n",
        "    }\n",
        "\n",
        "# classifier data params\n",
        "split_rate = 0.8  # train_test split\n",
        "batch_size_classifier = 16\n",
        "\n",
        "# encoder params\n",
        "enc_model = '1d_conv'  # 'spectogram' \n",
        "z_dim = 256  # output dim\n",
        "encoder_args = {\n",
        "    \"z_dim\": z_dim,\n",
        "    \"stride_sizes\": [5,4,2,2,2],\n",
        "    \"kernel_sizes\": [10,8,4,4,4],\n",
        "    \"n_filters\": [512,512,512,512,512],\n",
        "    \"activation\": tf.nn.leaky_relu\n",
        "    }\n",
        "\n",
        "\n",
        "# AR params\n",
        "# TODO: import accordingly given the model name in modularized fashion\n",
        "ar_model = 'GRU'  # 'transformer'\n",
        "c_dim = 512\n",
        "ar_args = {\n",
        "    'num_enc_layers': 5,\n",
        "    'num_heads': 8,\n",
        "    'z_dim': z_dim,\n",
        "    'dff': z_dim,\n",
        "    'dense_units': [z_dim, z_dim, c_dim],\n",
        "    'activation': tf.nn.tanh,\n",
        "    'maximum_position_encoding': data_generator_arguments['T'],\n",
        "    'rate': 0.1\n",
        "    }\n",
        "\n",
        "# training params\n",
        "epochs_cpc = 1  #500\n",
        "steps_per_epoch_cpc = 1  #100\n",
        "epochs_class = 1  #1000\n",
        "learning_rate = 1e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZGrft4plK7B"
      },
      "source": [
        "# Preprocess_data.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-x3C_YNBsrW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc0M7j90lP1v"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "import tensorflow.experimental.numpy as tfnp\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# import globals\n",
        "\n",
        "\n",
        "#### INPUT PIPELINE FUNCTIONS/GENERATORS\n",
        "\n",
        "def decode_audio(audio_path, original_sr, desired_sr, duration, max_duration=30):\n",
        "    \"\"\"\n",
        "    Loads and decodes wav file and applies sub- or supersampling to achieve a desired sampling rate. \n",
        "    Pads the audio tensor with zeros up to max_duration and then randomly takes a duration seconds long random crop.\n",
        "    \"\"\"\n",
        "\n",
        "    audio_binary = tf.io.read_file(audio_path)\n",
        "    audio, sr = tf.audio.decode_wav(audio_binary, desired_channels = 1, desired_samples = max_duration * original_sr)\n",
        "    audio = tf.image.random_crop(audio, size = (duration*sr,1))\n",
        "\n",
        "    if not desired_sr == original_sr:\n",
        "        audio = tfio.audio.resample(audio, original_sr, desired_sr)\n",
        "\n",
        "    return tf.squeeze(audio, axis=-1), desired_sr\n",
        "\n",
        "\n",
        "def batch_data_generator():\n",
        "    \"\"\"\n",
        "    Relies on a global argument dictionary \"data_generator_arguments\" containing the keys:\n",
        "\n",
        "    T:                  Number of time-steps (each being an audio window) used for prediction\n",
        "    k:                  Number of time-steps (each being an audio window) to predict\n",
        "    N:                  Number of negative samples (false/random prediction audio-windows)\n",
        "    original_sr:        Sampling rate of the audio files\n",
        "    desired_sr:         Sampling rate used for resampling the audio files (can reduce computational load but cuts high frequencies)\n",
        "    full_duration:      Length of audio files (shorter files get padded, longer files get cropped)\n",
        "    filepaths:          List of filepaths to wav files.\n",
        "\n",
        "    Negative samples are drawn only from other audio files in the batch as in [van den Oord et al 2018]. Batch size equals N.\n",
        "\n",
        "    Outputs a batch tensor of shape (batch_size, T +k*N, window_size, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    global data_generator_arguments\n",
        "\n",
        "    T = data_generator_arguments[\"T\"]\n",
        "    k = data_generator_arguments[\"k\"]\n",
        "    N = data_generator_arguments[\"N\"]\n",
        "    original_sr = data_generator_arguments[\"original_sr\"]\n",
        "    desired_sr = data_generator_arguments[\"desired_sr\"]\n",
        "    duration = data_generator_arguments[\"full_duration\"]\n",
        "    filepaths = data_generator_arguments[\"filepaths\"]\n",
        "    batch_size = N\n",
        "\n",
        "    window_size = duration*desired_sr/(T+k)\n",
        "    assert not window_size%1, f\"duration*sample rate and (T+k) must be divisible. Currently duration*sample_rate = {duration*desired_sr} and (T+k) = {T+k}\"\n",
        "    window_size = int(window_size)\n",
        "    \n",
        "    while True:\n",
        "\n",
        "        # get audio from randomly sampled paths, truncated to duration and resampled to desired sr\n",
        "        paths = random.sample(filepaths, batch_size)\n",
        "        songs = [decode_audio(path, original_sr, desired_sr, duration)[0] for path in paths]\n",
        "        \n",
        "        batch = []\n",
        "        for idx in range(batch_size):\n",
        "            samples = []\n",
        "            positive_sample = songs[idx]\n",
        "            positive_sample = tf.reshape(positive_sample, (1, T+k, window_size, 1))\n",
        "            samples.append(positive_sample)\n",
        "\n",
        "            # add a set of negative (not coming from index idx) sample audio windows of size (1,k,window_size,1)\n",
        "            for i, audio in enumerate(songs):\n",
        "                if i != idx:\n",
        "                    samples.append(tf.reshape(\n",
        "                        tensor = tf.image.random_crop(audio, size = [window_size * k]), \n",
        "                        shape  = (1, k, window_size, 1)))\n",
        "            \n",
        "            # get one sample with shape (1, T +k*N, window_size, 1)\n",
        "            batch.append(tf.concat(samples, axis = 1))\n",
        "\n",
        "        yield tf.concat(batch, axis= 0) # yield complete batch from single samples\n",
        "\n",
        "\n",
        "\n",
        "def create_cpc_ds():\n",
        "    \"\"\"\n",
        "    Uses a global dictionary \"data_generator_arguments\" to create a tf dataset from a generator that outputs batches already.\n",
        "\n",
        "    The data_generator_arguments dictionary has the following arguments:\n",
        "\n",
        "    T:                  Number of time-steps (each being an audio window) used for prediction\n",
        "    k:                  Number of time-steps (each being an audio window) to predict\n",
        "    N:                  Number of negative samples (false/random prediction audio-windows)\n",
        "    original_sr:        Sampling rate of the audio files\n",
        "    desired_sr:         Sampling rate used for resampling the audio files (can reduce computational load but cuts high frequencies)\n",
        "    full_duration:      Length of audio files (shorter files get padded, longer files get cropped)\n",
        "    filepaths:          List of filepaths to wav files.\n",
        "    \"\"\"\n",
        "\n",
        "    global data_generator_arguments\n",
        "    T = data_generator_arguments[\"T\"]\n",
        "    k = data_generator_arguments[\"k\"]\n",
        "    N = data_generator_arguments[\"N\"]\n",
        "    sampling_rate = data_generator_arguments[\"desired_sr\"]\n",
        "    batch_size = N\n",
        "    duration = data_generator_arguments[\"full_duration\"]\n",
        "    sr = data_generator_arguments[\"desired_sr\"]\n",
        "\n",
        "    # output shape of generator given the arguments\n",
        "    data_shape = (batch_size, T+k*N, int((duration*sr)/(T+k)), 1)\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_generator(\n",
        "        generator = batch_data_generator,\n",
        "        output_signature = tf.TensorSpec(data_shape, \n",
        "                                        dtype=tf.dtypes.float32,\n",
        "                                        name=None)\n",
        "                                        )\n",
        "    \n",
        "    train_ds = train_ds.prefetch(tf.data.AUTOTUNE).cache()\n",
        "\n",
        "    return train_ds\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8MczzCbkBkX"
      },
      "source": [
        "def create_tfds(inputs, targets, batch_size=None, buffer_size=None, prefetch_factor=None):\n",
        "    '''\n",
        "    Create an input pipeline from tf.dataset.\n",
        "    Adjusted to only take input as there are no labels for autoencoders.\n",
        "    '''\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
        "    if not buffer_size is None:\n",
        "        dataset = dataset.shuffle(buffer_size)\n",
        "    if not batch_size is None:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "    if not prefetch_factor is None:\n",
        "        dataset = dataset.prefetch(prefetch_factor)\n",
        "    return dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31w6mpbZBnum"
      },
      "source": [
        "# Extract_Embeddings.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8TJeEhIBxJh"
      },
      "source": [
        "### from a folder with embedding npy files, create a tf dataset with labels\n",
        "def get_embedding_datasets(embedding_path, embedding_folder, files):\n",
        "    #embedding_path = \"/content/gtzan_embeddings/\"\n",
        "    #embedding_folder = \"gtzan_embeddings/\"\n",
        "\n",
        "    gtzan_em_files = listdir(embedding_path)\n",
        "    gtzan_em_filepaths = [os.path.join(embedding_path, f) for f in files] # train files was created for training\n",
        "\n",
        "    embedding_data = [np.load(x) for x in gtzan_em_filepaths]\n",
        "\n",
        "    classes = np.array([\"blues\", \"reggae\", \"metal\", \"rock\", \"pop\", \"classical\", \"country\", \"disco\", \"jazz\", \"hiphop\"])\n",
        "\n",
        "    gtzan_onehot_labels = [tf.eye(10)[np.argwhere(classes == g.split(embedding_folder)[1].split(\".\")[0].split(\"__\")[-1])[0][0]] for g in gtzan_filepaths]\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((embedding_data, gtzan_onehot_labels))\n",
        "\n",
        "    return ds\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pUAktzkC1Il"
      },
      "source": [
        "### create embeddings (requires its own main script where all trained models are used to get embeddings)\n",
        "def create_embedding_dataset(model, iterations, gtzan_filepaths, gtzan_location, save_to):\n",
        "     #TODO: incorporate the arguments such that it can be reused\n",
        "\n",
        "    ######################### create a large dataset for training  ###############################################\n",
        "    iterations = 10  #number of times that 4 (duration) seconds are sampled from an audio clip\n",
        "    #gtzan_location = \"GTZAN/\"\n",
        "\n",
        "    #gtzan_files = listdir(\"/content/gtzan_songs/GTZAN/\")\n",
        "    #gtzan_filepaths = [os.path.join(\"/content/gtzan_songs/GTZAN/\", f) for f in gtzan_files]\n",
        "\n",
        "    original_sr = 22050\n",
        "    desired_sr = 4410       # should match the sample rate that CPC was trained with\n",
        "    duration = 4            # in seconds, should match the length of audio that CPC was trained with\n",
        "    segments = 30           # units, important to calculate segment_length (should match what T+k was during CPC training)\n",
        "    max_duration = 30       # in seconds\n",
        "    segment_length = int(duration*desired_sr/segments)\n",
        "\n",
        "    for i in range(0,iterations):\n",
        "\n",
        "        for fpath in gtzan_filepaths:\n",
        "            audio_binary = tf.io.read_file(fpath)\n",
        "            audio, sr = tf.audio.decode_wav(audio_binary, desired_channels = 1, desired_samples = max_duration * original_sr)\n",
        "\n",
        "            if not desired_sr == original_sr:\n",
        "                audio = tfio.audio.resample(audio, original_sr, desired_sr)\n",
        "\n",
        "            audio = tf.squeeze(audio, axis=-1)\n",
        "            audio = tf.image.random_crop(audio, size = (segments * segment_length,))\n",
        "            audio = tf.reshape(audio, (1,segments, segment_length, 1))\n",
        "\n",
        "            embedding = model.get_embedding(audio)\n",
        "            embedding = tf.squeeze(embedding, axis= 0)\n",
        "            save_to = \"/content/gtzan_embeddings/\"+ \"sample__\"+ str(i) + \"__\" + fpath.split(gtzan_location)[1].replace(\".wav\", \".npy\")\n",
        "            np.save(save_to, embedding.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKu5gcWllgpn"
      },
      "source": [
        "# encoder_models.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLx_9dwqlmCX"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Conv1DEncoder(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Encodes an input 1D sequence into an audio window embedding.\n",
        "\n",
        "    z_dim: size of embedding\n",
        "\n",
        "    stride_sizes: list of stride arguments for Conv1D layers\n",
        "    kernel_sizes: list of kernel size arguments for Conv1D layers\n",
        "    n_filters:    list of filter number arguments for Conv1D layers\n",
        "    activation:   activation function used in Conv1D layers and for output Dense layer. (e.g. \"relu\" or tf.nn.relu)\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self, z_dim, stride_sizes, kernel_sizes, n_filters, activation):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        s = stride_sizes\n",
        "        k = kernel_sizes\n",
        "        f = n_filters       \n",
        "\n",
        "        self.enc_layers = []\n",
        "\n",
        "        for l in range(len(f)):\n",
        "            self.enc_layers.append(tf.keras.layers.Conv1D(f[l],k[l],s[l]))\n",
        "            self.enc_layers.append(tf.keras.layers.BatchNormalization())\n",
        "            self.enc_layers.append(tf.keras.layers.Activation(activation))\n",
        "\n",
        "            #self.enc_layers.append(tf.keras.layers.LayerNormalization())\n",
        "\n",
        "        self.enc_layers.append(tf.keras.layers.Flatten())\n",
        "        self.enc_layers.append(tf.keras.layers.Dropout(0.1))\n",
        "        #self.enc_layers.append(tf.keras.layers.Dense(512))\n",
        "        #self.enc_layers.append(tf.keras.layers.Activation(activation))\n",
        "        #self.enc_layers.append(tf.keras.layers.Dropout(0.1))\n",
        "        self.enc_layers.append(tf.keras.layers.Dense(z_dim))\n",
        "        self.enc_layers.append(tf.keras.layers.Activation(activation))\n",
        "        \n",
        "    def call(self, x, training):\n",
        "        # input dim: [batch, T+K*N, window_size, 1]\n",
        "\n",
        "        for l in self.enc_layers:\n",
        "            try:\n",
        "                x = l(x, training)\n",
        "            except:\n",
        "                x = l(x)\n",
        "\n",
        "        # ouput dim:[batch, T+K*N, z]\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-P_F_6lmBMm"
      },
      "source": [
        "# autoregressive_models.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pfjaMe8mEaE"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# TODO: use transformer\n",
        "class GRU_Autoregressive(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    GRU RNN that takes a sequence of audio window embeddings and combines them into a context embedding.\n",
        "    c_dim: length of context embedding vector\n",
        "    '''\n",
        "\n",
        "    def __init__ (self, c_dim):\n",
        "        super(Autoregressive, self).__init__()\n",
        "        self.gru = tf.keras.layers.GRU(c_dim, name='ar_context',)\n",
        "        \n",
        "\n",
        "    def call (self, z_sequence):\n",
        "                                    # input dim: [batch, T, z]\n",
        "        return self.gru(z_sequence) # output dim:[batch, c]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFRPooRSEoud"
      },
      "source": [
        "#### TRANSFORMER LAYER CLASS AND FUNCTIONS\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)  # (1, position, d_model)\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable\n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "                                tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "                                tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "                                ])  \n",
        " \n",
        "\n",
        "\n",
        "class Trans_EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(Trans_EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_enc_layers, num_heads,\n",
        "                 z_dim, dff, dense_units, activation, maximum_position_encoding, rate=0.1):\n",
        "        '''\n",
        "        num_enc_layers: num. transformer encoder layers to be stacked\n",
        "        num_heads: num. sets of q,k,v\n",
        "        z_dim: z_dim\n",
        "        dff: num. units for first dense layer within encoder layer\n",
        "        dense_units: list of num. units for additional dense layers, last number is c_dim\n",
        "        activation: activation func. to use for additional dense layers\n",
        "        maximum_position_encoding: T in our case, max length of sequence\n",
        "        rate: dropout rate\n",
        "        '''\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.z_dim = z_dim\n",
        "        self.num_enc_layers = num_enc_layers\n",
        "\n",
        "        # embedding layer isn't needed as the input is already embedded\n",
        "        # self.embedding = tf.keras.layers.Embedding(input_vocab_size, z_dim)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                                self.z_dim)\n",
        "\n",
        "        self.enc_layers = [Trans_EncoderLayer(z_dim, num_heads, dff, rate)\n",
        "                        for _ in range(num_enc_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "        # additional dense layer and dropouts at the end\n",
        "        # TODO: using 1d_conv might make more sense but too much hyperparam\n",
        "        self.densenet = [[tf.keras.layers.Dense(n_l, activation), tf.keras.layers.Dropout(rate)] for n_l in dense_units]\n",
        "        self.densenet = [l for sublist in self.densenet for l in sublist]  # flatten\n",
        "        del self.densenet[-1]  # last dropout\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]  # T\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        # x = self.embedding(x)  # (batch_size, input_seq_len, z_dim)\n",
        "        x *= tf.math.sqrt(tf.cast(self.z_dim, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_enc_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)  # (batch_size, input_seq_len, z_dim)\n",
        "\n",
        "        x = tf.keras.layers.Flatten()(x)  # (batch_size, input_seq_len*z_dim)\n",
        "        for i in range(len(self.densenet)):\n",
        "            if i % 2 == 0:\n",
        "                x = self.densenet[i](x)  # dense layer\n",
        "            else:\n",
        "                x = self.densenet[i](x, training)  # dropout     \n",
        "        \n",
        "        return x  # (batch_size, c_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVuUpUpgmH_y"
      },
      "source": [
        "# cpc_model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMx95EqumNh7"
      },
      "source": [
        "import tensorflow as tf\n",
        "# from autoreg_model import Autoregressive\n",
        "# from autoencoder_model import Encoder\n",
        "\n",
        "\n",
        "class Predict_z(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Layer that uses the context embedding c_t to predict K (future) embeddings\n",
        "    '''\n",
        "\n",
        "    def __init__(self, z_dim, K, mixed_precision=False):\n",
        "        super(Predict_z, self).__init__()\n",
        "        \n",
        "        # input_dim: [batch, c_dim]\n",
        "        self.transform_layers = []\n",
        "\n",
        "        if mixed_precision:\n",
        "            self.z_dtype = tf.float16\n",
        "        else:\n",
        "            self.z_dtype= tf.float32\n",
        "\n",
        "        # one linear layer for each future time-step\n",
        "        for k in tf.range(K):  \n",
        "            self.transform_layers.append(tf.keras.layers.Dense(z_dim))\n",
        "\n",
        "    def call(self, c_t):\n",
        "        \n",
        "        z_pred = tf.TensorArray(self.z_dtype, size=len(self.transform_layers))\n",
        "\n",
        "        for l,layer in enumerate(self.transform_layers):\n",
        "            # apply linear projection layer for each k\n",
        "            z_pred = z_pred.write(l, layer(c_t))                        \n",
        "\n",
        "        z_pred_t = z_pred.stack()\n",
        "        # output_dim: [batch, K, z]\n",
        "        return tf.transpose(z_pred_t, perm=[1,0,2])                      \n",
        "\n",
        "def compute_f(z, z_pred):\n",
        "    '''\n",
        "    Compute f-scores following eq(3) in the paper to be batch (K x N) matrices.\n",
        "    Computes similarity (f-)scores as the exp of the dot product of two embeddings. \n",
        "    First column of the returned f-score matrix is the postive sample.\n",
        "    '''                                                                                         \n",
        "                                                                                    # z_pred input dim: [batch, K, z]\n",
        "                                                                                    # z input dim:      [batch, K, N, z]\n",
        "    z = tf.expand_dims(z, axis=-2)                                                  # -> [batch, K, N, 1, z]\n",
        "                                                                        \n",
        "    pred = tf.repeat(z_pred, repeats=z.shape[2], axis=-2)                           # -> [batch, K*N, z]\n",
        "    pred = tf.reshape(pred, shape=[z.shape[0],z.shape[1],z.shape[2],z.shape[-1]])   # -> [batch, K, N, z]\n",
        "    pred = tf.expand_dims(pred, axis=-1)                                            # -> [batch, K, N, z, 1]\n",
        "\n",
        "    dot_prod = tf.linalg.matmul(z, pred)                                            # -> [batch, K, N, 1, 1]\n",
        "    #cosine_similarity = dot_prod/(tf.norm(z)*tf.norm(pred))\n",
        "    dot_prod = tf.squeeze(dot_prod, axis=[-2,-1])                                   # -> [batch, K, N]\n",
        "    f_mat = tf.exp(dot_prod)\n",
        "                                                                                    # output dim: [batch, K, N]\n",
        "    return f_mat \n",
        "\n",
        "\n",
        "class CPC(tf.keras.models.Model):\n",
        "    '''\n",
        "    Full Contrastive Predictive Coding Model.\n",
        "\n",
        "    n_observations:     number of subsequent windows of audio used for prediction\n",
        "    n_future:           number of future audio windows to predict\n",
        "    n_negative_samples: number of random negative samples \n",
        "    z_dim:              audio window encoding size\n",
        "    encoder_args:       argument dictionary for Encoder model\n",
        "    '''\n",
        "\n",
        "    def __init__ (self, n_observations, n_future, n_samples, z_dim, c_dim, Encoder, Autoregressive_Model, Predict_z, encoder_args, ar_args, m_precision):\n",
        "        super(CPC, self).__init__()\n",
        "\n",
        "        self.T = n_observations\n",
        "        self.K = n_future\n",
        "        self.N = n_samples\n",
        "\n",
        "        self.z = z_dim\n",
        "        self.c = c_dim\n",
        "\n",
        "        self.g_enc = Encoder(**encoder_args)\n",
        "        self.g_ar = Autoregressive_Model(**ar_args)\n",
        "        self.p_z = Predict_z(z_dim=self.z, K=self.K, mixed_precision = m_precision)\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "\n",
        "        z_t = tf.keras.layers.TimeDistributed(self.g_enc)(x, training= False)\n",
        "        c_T = self.g_ar(z_t)\n",
        "\n",
        "        return c_T\n",
        "\n",
        "    def call(self, x, training=False):  \n",
        "                                                                                                # input dim: [batch, T+K*N, window_size, 1]\n",
        "        # Obtain Embeddings for T+k*N time windows of length d\n",
        "        z_t = tf.keras.layers.TimeDistributed(self.g_enc)(x, training=training)                         # -> [batch, T+K*N, z_dim]\n",
        "        \n",
        "        # Split into current observation embeddings and (positive and negative) future embeddings\n",
        "        z_obs = z_t[:, :self.T]                                                                         # -> [batch,   T, z]\n",
        "        z_future = z_t[:, self.T:]                                                                      # -> [batch, K*N, z]\n",
        "        z_future = tf.reshape(z_future, [-1, self.K, self.N, self.z])                                   # -> [batch, K, N, z]\n",
        "\n",
        "        # Obtain context embedding vector for T encoded time-windows\n",
        "        c_T = self.g_ar(z_obs)                                                                          # -> [batch, c]\n",
        "\n",
        "        # Linearly project context vector to make predictions of the future encoded time-windows\n",
        "        z_pred = self.p_z(c_T)                                                                          # -> [batch, K, z]\n",
        "\n",
        "        # Compute f matrix in which the first column is the f-scores for the positive sample\n",
        "\n",
        "        f_mat = compute_f(z_future, z_pred)                                                     #output dim: [batch, K, N]\n",
        "\n",
        "        return f_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnaNGf09-wt2"
      },
      "source": [
        "# InfoNCE_loss.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPILFCYLTfga"
      },
      "source": [
        "class InfoNCE (tf.keras.losses.Loss):\n",
        "    '''\n",
        "    Compute InfoNCE loss given a batch of f matrices with dim (K x N)\n",
        "    '''\n",
        "\n",
        "    def __call__(self, f):\n",
        "                                                         # input dim: [batch, K, N]\n",
        "        denominator = tf.reduce_sum(f, axis=2)           # -> [batch, K]\n",
        "        losses = - tf.math.log(f[:,:,0] / denominator)  # first column is the positive k predictions\n",
        "        # TODO: weighted avg. instead of uniform avg.\n",
        "        loss = tf.reduce_mean(losses, axis=None)         # Take mean loss over batch_size and K\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8WRwvAwmY7a"
      },
      "source": [
        "# classifier_model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzSqtT-hmYZD"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "### classification model\n",
        "c_dim = 512 # needed as an argument\n",
        "\n",
        "def get_classifier(c_dim, num_classes):\n",
        "\n",
        "    embedding_inputs = tf.keras.Input(shape=(c_dim))\n",
        "    x = tf.keras.layers.Dense(64, activation=\"relu\")(embedding_inputs)\n",
        "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "    outputs = tf.keras.layers.Dense(10, activation = \"softmax\")(x)\n",
        "    model = tf.keras.Model(inputs= embedding_inputs, outputs=outputs, name=\"music_classifier\")\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j3Smkx6mp7f"
      },
      "source": [
        "# training.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqNK2ss8mrQp"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, ds, loss_function, optimizer, \n",
        "               steps_per_epoch, train_loss_metric=None, mixed_precision=False):\n",
        "\n",
        "    for batch in ds.take(steps_per_epoch):\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            prediction = model(batch,training=True)\n",
        "            loss = loss_function(prediction)\n",
        "\n",
        "            if mixed_precision:\n",
        "                loss = optimizer.get_scaled_loss(loss) # scaled loss for mixed precision training\n",
        "        \n",
        "        gradients = tape.gradient(loss, model.trainable_variables) # get (scaled) gradients\n",
        "\n",
        "        if mixed_precision:\n",
        "            gradients = optimizer.get_unscaled_gradients(scaled_gradients) # get unscaled gradients from scaled gradients\n",
        "\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # apply unscaled gradients\n",
        "\n",
        "        # update metric\n",
        "        train_loss_metric.update_state(loss)\n",
        "\n",
        "\n",
        "def eval_metric(metric, val_list):\n",
        "    result = metric.result()\n",
        "    metric.reset_states()\n",
        "    val_list.append(result)\n",
        "    return result\n",
        "\n",
        "\n",
        "# TODO: eventually do training in a main loop and not with a single func\n",
        "def train_cpc(epochs, model, ds_train, ds_test, loss_function, optimizer, steps_per_epoch,\n",
        "                         train_loss_metric=None, train_acc_metric=None, test_loss_metric=None, test_acc_metric=None,\n",
        "                         PATH=False):\n",
        "    '''\n",
        "    Call appropriate train and test steps depending on the mode.\n",
        "    :param epochs: int, number of epochs to train\n",
        "    :param model: tf.keras.model, model to train\n",
        "    :param ds_train: tf.keras.data.dataset, dataset to train on\n",
        "    :param ds_test: tf.keras.data.dataset, dataset to test on\n",
        "    :param loss_function: tf.keras.loss, loss function to use\n",
        "    :param optimizer: tf.keras.optimizer, optimizer to use\n",
        "    :param steps_per_epoch: int, number of batch to feed per epoch\n",
        "    :param train_loss_metric: tf.keras.metric\n",
        "    :param test_loss_metric: tf.keras.metric\n",
        "    :param PATH: str, path to save to trained model weights\n",
        "    :return: list of list, 4 metrics for all epochs\n",
        "    '''\n",
        "\n",
        "    # arrays to save results for all epochs\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for e in range(epochs):\n",
        "        # Train\n",
        "        train_step_cpc(model, ds_train, loss_function, optimizer, steps_per_epoch, train_loss_metric)\n",
        "        \n",
        "        eval_metric(train_loss_metric, train_losses)\n",
        "\n",
        "        if train_acc_metric is not None:\n",
        "            eval_metric(train_acc_metric, train_accuracies)\n",
        "\n",
        "    # Save model weights\n",
        "    if PATH:\n",
        "        from datetime import datetime\n",
        "        now = datetime.now()\n",
        "        save_to = PATH + str(now)[:-10] + \".h5\"\n",
        "        model.save_weights(save_to, overwrite=False)\n",
        "\n",
        "    return train_losses, train_accuracies, test_losses, test_accuracies\n",
        "\n",
        "# formerly main eval train with mode (now we only train CPC with a custom train function and the classifier with model.fit()\n",
        "def train_cpc(cpc_model, train_ds, loss_function, optimizer, epochs, steps_per_epoch, train_loss_metric, mixed_precision, save_to)\n",
        "\n",
        "    epochs = 10\n",
        "    for e in range(epochs):\n",
        "        train_step(cpc_model, train_ds, loss_function, optimizer, 100, train_loss_metric, mixed_precision)\n",
        "\n",
        "        train_losses.append(train_loss_metric.result().numpy())\n",
        "\n",
        "        print(f\"Episode:{e}    loss: {train_losses[-1]}\")\n",
        "\n",
        "        train_loss_metric.reset_states()\n",
        "\n",
        "    now = datetime.now()\n",
        "\n",
        "    # save model parameters to .h5 file. Can afterwards be loaded with cpc.load_weights(load_from)\n",
        "    save_to = save_to + str(now)[:-10] + \".h5\"\n",
        "    cpc_model.save_weights(save_to, overwrite=False)\n",
        "\n",
        "    # save loss array for later visualization\n",
        "    losses_array = np.array(train_losses)\n",
        "    np.save(save_to.replace(\".h5\",\".npy\"),losses_array)\n",
        "\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGZi3YDdHQOH"
      },
      "source": [
        "# train_classifiers.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHZcxhMMJsnS"
      },
      "source": [
        "def plot_classifier_training(history, epochs, save_plot_as):\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(0, epochs), history.history[\"loss\"], label=\"train_loss\")\n",
        "    plt.plot(np.arange(0, epochs), history.history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.plot(np.arange(0, epochs), history.history[\"accuracy\"], label=\"train_acc\")\n",
        "    plt.plot(np.arange(0, epochs), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "    plt.title(\"Training Loss and Accuracy\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Loss/Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.savefig(save_plot_as))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdFpYXVtHTR4"
      },
      "source": [
        "EPOCHS = 10000\n",
        "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "c_dim = 512\n",
        "\n",
        "# TODO: get list of tuples (train_ds, test_ds) for each embedding  via create_embedding_dataset(filepaths ... )\n",
        "\n",
        "#datasets = [create_embedding_dataset(...) for path in embedding_paths]\n",
        "\n",
        "# create save_plot_as list of filenames/paths for the plots\n",
        "\n",
        "# iterate over datasets list and do the following. also iterate (with zip) over the save_plot_as list.\n",
        "\n",
        "train_dataset = ds[0]\n",
        "test_dataset  = ds[1]\n",
        "model = get_classifier(c_dim, 10)\n",
        "model.compile(optimizer = optimizer, \n",
        "                loss = loss, \n",
        "                metrics=[\"accuracy\"])\n",
        "history = model.fit(train_dataset, epochs = EPOCHS, batch_size = 32, validation_data = test_dataset) # add additional arguments\n",
        "\n",
        "### plot the history\n",
        "plot_classifier_training(history, EPOCHS, plotname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHSHQf3ylNpp"
      },
      "source": [
        "# Main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WINQkBAxgtAc"
      },
      "source": [
        "### (A) Train the CPC model ###\n",
        "T = data_generator_arguments[\"T\"]\n",
        "k = data_generator_arguments[\"k\"]\n",
        "N = data_generator_arguments[\"N\"]\n",
        "batch_size = N\n",
        "\n",
        "mixed_precision = False\n",
        "if mixed_precision:\n",
        "    tf.keras.mixed_precision.set_global_policy('mixed_float16') # use mixed precision training (on V100 supposedly a 3x performance boost + double memory)\n",
        "else:\n",
        "    tf.keras.mixed_precision.set_global_policy('float32')\n",
        "\n",
        "# Generate a dataset\n",
        "train_ds_cpc = create_cpc_ds()\n",
        "\n",
        "\n",
        "# Define 3 design components\n",
        "# Model\n",
        "\n",
        "# import respective Encoder class as Encoder, import Autoregressive class as Autoregressive, import encoder_args and ar_args that are associated with them.\n",
        "\n",
        "cpc = CPC(T, k, N, z_dim, c_dim, Encoder, Autoregressive, encoder_args, ar_args, mixed_precision)\n",
        "\n",
        "# load trained model\n",
        "if load_path:\n",
        "    cpc.load_weights(load_path)\n",
        "\n",
        "# Loss\n",
        "infonce = InfoNCE()\n",
        "train_loss_metric_cpc = tf.keras.metrics.Mean('train_loss_CPC')\n",
        "# Optimizer\n",
        "adam = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "\n",
        "# Training\n",
        "res_train_loss_cpc, *_ = main_train_eval_loop('CPC', epochs_cpc, cpc, ds_train_cpc, None, infonce, adam, steps_per_epoch=steps_per_epoch_cpc,\n",
        "                                          train_loss_metric=train_loss_metric_cpc, PATH=weight_path)\n",
        "######\n",
        "\n",
        "\n",
        "\n",
        "# Define 3 design components\n",
        "# Model\n",
        "num_classes = list(gtzan_train.take(1).as_numpy_iterator())[0][1].shape[1]\n",
        "classi1 = Classifier(num_classes)\n",
        "# Loss\n",
        "cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "train_acc_metric_classi1 = tf.keras.metrics.CategoricalAccuracy('train_accuracy_classi1')\n",
        "train_loss_metric_classi1 = tf.keras.metrics.Mean('train_loss_classi1')\n",
        "test_acc_metric_classi1 = tf.keras.metrics.CategoricalAccuracy('test_accuracy_classi1')\n",
        "test_loss_metric_classi1 = tf.keras.metrics.Mean('test_loss_classi1')\n",
        "\n",
        " \n",
        "# Training and testing\n",
        "res_train_loss_c1, res_train_acc_c1, res_test_loss_c1, res_test_acc_c1 = main_train_eval_loop(\n",
        "    'classifier', epochs_class, classi1, gtzan_train, gtzan_test, cce, adam, steps_per_epoch=8, # TODO: steps for classi?\n",
        "    train_loss_metric=train_loss_metric_classi1, train_acc_metric=train_acc_metric_classi1,\n",
        "     test_loss_metric=test_loss_metric_classi1, test_acc_metric=test_acc_metric_classi1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607,
          "referenced_widgets": [
            "408dbc681281422ab3cf95cb8e15079e",
            "c0b7d277bd124b0683b0e5f52ed791a2",
            "d9b002eb13f54326b0f8d2a0a9b4d36a",
            "1f37a9612d5d481bab9fafc8fed89570",
            "a237664718a245558453325759cba9ed",
            "bd1245c0383245309e6ecf5d2177760d",
            "7c64bd66dbea4c2eb5637d36c94bcb93",
            "2aa831fdddb54c91b9be74175cbc664e",
            "cb6e8e0ea67444238f796a1977426ae4",
            "76d91dac564c4deab5c8278105844107",
            "df0fbfdb0d7f428abd5f5e83a5308b31",
            "8778a44bf13e48099cd74db6cab75380",
            "68585baed1384c729dc9657892bf5931",
            "77c222efbaf54e0194834cf2714afb8f",
            "b99f81b07134496f95a2c531c3f1259d",
            "908db350dbc6475bb4f4d105ed448c99",
            "4c778fb61a444729ad05950f844f85b2",
            "783eb0f618e149568bc2f78c79cbaa4d",
            "a3db588eaa5745c09c3ec6bd5191bc7f",
            "cd8fd2d21e584d86acb2b50a16c7fc8b",
            "f9c3e6abfdfd44f597c655bae5de07c1",
            "61aa1736775c473f8cc4d24f413059f9",
            "6eda82f853b04d25b5bb16081dd6a925",
            "5106a82d067f4839be0f57f09c534d73"
          ]
        },
        "id": "fLQi0w4mpXwV",
        "outputId": "ab630c7a-433a-4603-e2e9-6a2d3a42ab70"
      },
      "source": [
        "### (D,E) Classify with learned features from CPC model\n",
        "# Get features using trained CPC\n",
        "import tensorflow_datasets as tfds\n",
        "gtzan_ds = tfds.load('gtzan', split='train', as_supervised=True).shuffle(1024).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "gtzan_ds_train = gtzan_ds.take(int(1000*split_rate))\n",
        "gtzan_ds_test = gtzan_ds.skip(int(1000*split_rate))\n",
        "\n",
        "\n",
        "def get_embedded_ds(ds, model):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for audio, label in ds:\n",
        "        audio = audio[:(T+k)*d]  # if GTZAN happends to be longer\n",
        "        audio = tf.reshape(audio, (1, T+k, d, 1))  # \n",
        "        print(audio.shape)\n",
        "        # TODO: Value for attr 'T' of int64 is not in the list of allowed values:\n",
        "        features.append(model.get_embedding(audio))\n",
        "        labels.append(label)\n",
        "    print(\"features, labels\")\n",
        "    print(np.array(features).shape, np.array(labels).shape)\n",
        "    return tf.data.Dataset.from_tensor_slices((features,labels))\n",
        "\n",
        "\n",
        "cpc_train_features = get_embedded_ds(gtzan_ds_train, cpc).batch(batch_size)\n",
        "cpc_test_features = get_embedded_ds(gtzan_ds_test, cpc).batch(batch_size)\n",
        "\n",
        "# Design components\n",
        "# model\n",
        "classi2 = Classifier(num_classes)\n",
        "# metrics\n",
        "train_acc_metric_classi2 = tf.keras.metrics.CategoricalAccuracy('train_accuracy_classi2')\n",
        "train_loss_metric_classi2 = tf.keras.metrics.Mean('train_loss_classi2')\n",
        "test_acc_metric_classi2 = tf.keras.metrics.CategoricalAccuracy('test_accuracy_classi2')\n",
        "test_loss_metric_classi2 = tf.keras.metrics.Mean('test_loss_classi2')\n",
        "\n",
        "# Training and testing\n",
        "res_train_loss_c2, res_train_acc_c2, res_test_loss_c2, res_test_acc_c2 = main_train_eval_loop(\n",
        "    'classifier', epochs_class, classi2, cpc_train_features, cpc_test_features, cce, adam, steps_per_epoch=batch_size_classifier,  # TODO: steps for classi?\n",
        "    train_loss_metric=train_loss_metric_classi2, train_acc_metric=train_acc_metric_classi2,\n",
        "     test_loss_metric=test_loss_metric_classi2, test_acc_metric=test_acc_metric_classi2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset gtzan/1.0.0 (download: 1.14 GiB, generated: 3.71 GiB, total: 4.85 GiB) to /root/tensorflow_datasets/gtzan/1.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "408dbc681281422ab3cf95cb8e15079e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb6e8e0ea67444238f796a1977426ae4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c778fb61a444729ad05950f844f85b2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-64f170279298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Get features using trained CPC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgtzan_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gtzan'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mgtzan_ds_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgtzan_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msplit_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgtzan_ds_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgtzan_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msplit_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    385\u001b[0m           self._download_and_prepare(\n\u001b[1;32m    386\u001b[0m               \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m               download_config=download_config)\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m           \u001b[0;31m# NOTE: If modifying the lines below to put additional information in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     super(GeneratorBasedBuilder, self)._download_and_prepare(\n\u001b[1;32m   1023\u001b[0m         \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    960\u001b[0m         prepare_split_kwargs)\n\u001b[1;32m    961\u001b[0m     for split_generator in self._split_generators(\n\u001b[0;32m--> 962\u001b[0;31m         dl_manager, **split_generators_kwargs):\n\u001b[0m\u001b[1;32m    963\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/audio/gtzan/gtzan.py\u001b[0m in \u001b[0;36m_split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     93\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_split_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;34m\"\"\"Returns SplitGenerators.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mdl_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"genres\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_DOWNLOAD_URL\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"genres\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"genres\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# There is no predefined train/val/test split for this dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36mdownload_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_map_promise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_extract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_or_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36m_map_promise\u001b[0;34m(map_fn, all_inputs)\u001b[0m\n\u001b[1;32m    634\u001b[0m   \u001b[0;34m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0mall_promises\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_promises\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait promises\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    634\u001b[0m   \u001b[0;34m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0mall_promises\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_promises\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait promises\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/promise/promise.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m# type: (Optional[float]) -> T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDEFAULT_TIMEOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_settled_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/promise/promise.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;31m# type: (Optional[float]) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/promise/promise.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(cls, promise, timeout)\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpromise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# type: (Promise, Optional[float]) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0masync_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpromise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/promise/async_.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, promise, timeout)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# fulfilled or rejected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdrain_queues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/promise/schedulers/immediate.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, promise, timeout)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpromise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_then\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mon_resolve_or_reject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_resolve_or_reject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mwaited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwaited\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timeout\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHLrB4t5u5lM"
      },
      "source": [
        "for i,l in gtzan_ds.take(1):\n",
        "    print(i,l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi032uXWxNAF"
      },
      "source": [
        "# TODO: automatic result and figure saving\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(ncols=2)\n",
        "\n",
        "ax[0].plot(np.array(res_train_loss_c1), color='r')\n",
        "ax[0].plot(np.array(res_test_loss_c1), color='b')\n",
        "ax[0].set(title='cce loss')\n",
        "ax[1].plot(np.array(res_train_acc_c1), color='r')\n",
        "ax[1].plot(np.array(res_test_acc_c1), color='b')\n",
        "ax[1].set(title='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4EbRqcH1wpK"
      },
      "source": [
        "while True:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
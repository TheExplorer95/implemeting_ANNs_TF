{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "official.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b98a69c855234882a65b8a4150f348b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_728500ddf8bb416d99176e34492b4b5a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fe8a17bf244846358ce6fe5f92a76f12",
              "IPY_MODEL_6043905dc16f43ce97f06ceabfaa2c00"
            ]
          }
        },
        "728500ddf8bb416d99176e34492b4b5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fe8a17bf244846358ce6fe5f92a76f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6d1a31ff352840d79655cb6c4e0b615e",
            "_dom_classes": [],
            "description": "Dl Completed...: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9aeef0ebc3de4a8286283617764d77f7"
          }
        },
        "6043905dc16f43ce97f06ceabfaa2c00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cc5855476a344208a3b3d2b898b962b5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/? [10:52&lt;00:00, 326.01s/ url]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7aa890101f0a4272ab7c3ee9a02b7676"
          }
        },
        "6d1a31ff352840d79655cb6c4e0b615e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9aeef0ebc3de4a8286283617764d77f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc5855476a344208a3b3d2b898b962b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7aa890101f0a4272ab7c3ee9a02b7676": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f65a390c9b44157bf334fa5307d6184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2760ec8674ac4265870c8b63e810b209",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e49d5aad4d1c431692e26d35ef0bc6c7",
              "IPY_MODEL_52096dc5e8bb406f8dd65e2c50a6c5e4"
            ]
          }
        },
        "2760ec8674ac4265870c8b63e810b209": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e49d5aad4d1c431692e26d35ef0bc6c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_09027621bb7b4994bb286fca52ec8e70",
            "_dom_classes": [],
            "description": "Dl Size...: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30edf5b6354447869292292523a53c35"
          }
        },
        "52096dc5e8bb406f8dd65e2c50a6c5e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3a77a0ef98bb4d44a6580a220f64c5be",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2180/? [10:51&lt;00:00,  3.34 MiB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b90b82f066904074a47aad1a244d35be"
          }
        },
        "09027621bb7b4994bb286fca52ec8e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30edf5b6354447869292292523a53c35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3a77a0ef98bb4d44a6580a220f64c5be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b90b82f066904074a47aad1a244d35be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9fc4aa180784377ad464fd807f6dcd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_be60987cbab04b019175dfacdd246c45",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_41de04ac05884c29a73b9e482080144b",
              "IPY_MODEL_6faa844540d143a6a1a89dec4e09b36a"
            ]
          }
        },
        "be60987cbab04b019175dfacdd246c45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41de04ac05884c29a73b9e482080144b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_186df641e73c4c59b4501fccdf014d80",
            "_dom_classes": [],
            "description": "Extraction completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e80dad1bb9a147a494e5150ceaa2e4ab"
          }
        },
        "6faa844540d143a6a1a89dec4e09b36a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_00f4aa9f4142460f8f8defafe8bb3459",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [10:51&lt;00:00, 651.94s/ file]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c71f6982a6974d88aad8d9107bb9650f"
          }
        },
        "186df641e73c4c59b4501fccdf014d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e80dad1bb9a147a494e5150ceaa2e4ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00f4aa9f4142460f8f8defafe8bb3459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c71f6982a6974d88aad8d9107bb9650f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e6a7aa406979447087b4efd7204e8e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_493f6a67ea3c4b94ba570c820b28e9f1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_63aaeda1837f4b72af121327c312f92f",
              "IPY_MODEL_aad149381ea24039b3de0c39afc7c0b7"
            ]
          }
        },
        "493f6a67ea3c4b94ba570c820b28e9f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "63aaeda1837f4b72af121327c312f92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c1853f4b52684d3cab84a886738c54f8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6b3461e6a02f4e31b599b7d822d27eca"
          }
        },
        "aad149381ea24039b3de0c39afc7c0b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4367869afb1942af9be5197947e0d75f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000/0 [03:36&lt;00:00,  4.60 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e9d049a635bd42c094b892ff0dcc6467"
          }
        },
        "c1853f4b52684d3cab84a886738c54f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6b3461e6a02f4e31b599b7d822d27eca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4367869afb1942af9be5197947e0d75f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e9d049a635bd42c094b892ff0dcc6467": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be778ca54a904522baa4e0e6b5007d30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_27c1a4ecb55743b69785cc64870cc5fe",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ee35d855f0ea472bb2b594c21f6841b1",
              "IPY_MODEL_d603ba16b640485d97581c4f2cb5bd09"
            ]
          }
        },
        "27c1a4ecb55743b69785cc64870cc5fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee35d855f0ea472bb2b594c21f6841b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4d50d6dc7ab6475e90747cc0712b3e6d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 998,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2134888bda224be69616b7271764922e"
          }
        },
        "d603ba16b640485d97581c4f2cb5bd09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0eaf2117ebce46e8aca5f415d223d028",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 998/1000 [01:30&lt;00:00,  9.70 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_261f5dec0ab54107bc56d25ee5b8dbf1"
          }
        },
        "4d50d6dc7ab6475e90747cc0712b3e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2134888bda224be69616b7271764922e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0eaf2117ebce46e8aca5f415d223d028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "261f5dec0ab54107bc56d25ee5b8dbf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxCLAomZiOf7",
        "outputId": "3721f0e5-9ae7-443e-c680-096359a8f053"
      },
      "source": [
        "!pip install tensorflow_io\n",
        "!pip install pydub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_io in /usr/local/lib/python3.7/dist-packages (0.17.0)\n",
            "Requirement already satisfied: tensorflow<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io) (2.4.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.12.4)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.15.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.4.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.3.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.10.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.36.2)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.32.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.7.4.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.10.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.4.1)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (54.1.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.3.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (4.7.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.7.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.4.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N85WhNVrgy99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd58484-9f6b-49d6-d8a5-b347df0db86f"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# import sys\n",
        "# sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/WS2021/ANN')\n",
        "print(\"lel\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "960cjYFmgyim"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# import globals\n",
        "# import preprocess_data\n",
        "# from contrPredCod_model import CPC, InfoNCE\n",
        "# from classifier_model import Classifier\n",
        "# from training import main_train_eval_loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4BgNxw1lqhW"
      },
      "source": [
        "# globals.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpz-zwWblr4y"
      },
      "source": [
        "def initialize():\n",
        "    global data_generator_arguments\n",
        "    data_generator_arguments = {}\n",
        "\n",
        "    global encoder_args\n",
        "    encoder_args = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6Q4Du8eoofQ"
      },
      "source": [
        "# hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFyfX_SwgkGJ"
      },
      "source": [
        "### Hyperparameters ###\n",
        "# data path\n",
        "cwd = '/content/drive/MyDrive/Colab Notebooks/WS2021/ANN/final'\n",
        "cpc_data_path = cwd+\"/data/2000songs.zip (Unzipped Files)/2000songs\"  # cpc train data\n",
        "files = os.listdir(cpc_data_path)\n",
        "filepaths = [os.path.join(cpc_data_path, f) for f in files]\n",
        "gtzan_feature_path = cwd+'/data/gtzan/features_30_sec.csv'  # baseline features\n",
        "weight_path = cwd+'/model/cpc'  # where to save weights\n",
        "load_path = False  # from where to load weights\n",
        "\n",
        "# CPC data params\n",
        "# TODO: globals.init if using python scipts and importing\n",
        "initialize()  # init the global variable\n",
        "data_generator_arguments = {\n",
        "    \"T\": 20,  # timestep\n",
        "    \"k\": 10,  # timestep\n",
        "    \"N\": 4,  # number\n",
        "    \"full_duration\": 5,  # sec\n",
        "    \"original_sr\": 22050,  # Hz\n",
        "    \"desired_sr\": 4410,  # Hz\n",
        "    \"filepaths\": filepaths\n",
        "    }\n",
        "\n",
        "# classifier data params\n",
        "split_rate = 0.8  # train_test split\n",
        "batch_size_classifier = 16\n",
        "\n",
        "# encoder params\n",
        "z_dim = 256  # output dim\n",
        "encoder_args = {\n",
        "    \"z_dim\"          : z_dim,\n",
        "    \"stride_sizes\": [5,4,2,2,2],\n",
        "    \"kernel_sizes\": [10,8,4,4,4],\n",
        "    \"n_filters\":    [512,512,512,512,512],\n",
        "    \"activation\":   tf.nn.leaky_relu\n",
        "}\n",
        "\n",
        "\n",
        "# AR params\n",
        "c_dim = 512\n",
        "\n",
        "# training params\n",
        "epochs = 2\n",
        "learning_rate = 1e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZGrft4plK7B"
      },
      "source": [
        "# Preprocess_data.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc0M7j90lP1v"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "import tensorflow.experimental.numpy as tfnp\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# import globals\n",
        "\n",
        "\n",
        "### FMA data\n",
        "def decode_audio(audio_path, original_sr, desired_sr, duration):\n",
        "    \"\"\"\n",
        "    Loads and decodes wav file and applies resampling.\n",
        "    Truncates or pads audio to desired length.\n",
        "\n",
        "    audio_path: str, path of the audio data to load\n",
        "    original_sr: float, original sampling rate in Hz\n",
        "    desired_sr: float, desired sampling rate to which the data should be resampled\n",
        "    duration: float, desired duration of data in sec\n",
        "    return: (tf.array, int), 1-d audio file and the desired sampling rate\n",
        "    \"\"\"\n",
        "\n",
        "    audio_binary = tf.io.read_file(audio_path)\n",
        "    audio, sr = tf.audio.decode_wav(audio_binary,\n",
        "                                    # mono sound\n",
        "                                    desired_channels=1,\n",
        "                                    # crop to get a desired duration\n",
        "                                    desired_samples=duration * original_sr)\n",
        "    # if resampling is needed\n",
        "    if not desired_sr == original_sr:\n",
        "        audio = tfio.audio.resample(audio, original_sr, desired_sr)\n",
        "    return tf.squeeze(audio, axis=-1), desired_sr\n",
        "\n",
        "\n",
        "def gen_batch_CPC():\n",
        "    \"\"\"\n",
        "    Yield a single batch of data by randomly selecting different audios.\n",
        "    Per default, a single batch is a permutation in which every selected data\n",
        "    is used as a postive sample once and other as negative samples.\n",
        "\n",
        "    T: int, #timesteps used for c_t\n",
        "    k: int, #timesteps to predict\n",
        "    N: int, number of samples (1 positive + N-1 negative samples)\n",
        "    original_sr: sampling rate of original audio files\n",
        "    desired_sr: sampling rate as input to CPC (used for subsampling)\n",
        "    duration: assumed full duration of an audio file (30s, files get truncated or padded with zeros)\n",
        "    filepaths: list of all filepaths to all audio files\n",
        "    \"\"\"\n",
        "\n",
        "    # used to get arguments to a generator\n",
        "    T = data_generator_arguments[\"T\"]\n",
        "    k = data_generator_arguments[\"k\"]\n",
        "    N = data_generator_arguments[\"N\"]\n",
        "    original_sr = data_generator_arguments[\"original_sr\"]\n",
        "    desired_sr = data_generator_arguments[\"desired_sr\"]\n",
        "    duration = data_generator_arguments[\"full_duration\"]\n",
        "    filepaths = data_generator_arguments[\"filepaths\"]\n",
        "    batch_size = N\n",
        "    d = int((duration * desired_sr) / (T + k))  # window size (length of a single segment)\n",
        "\n",
        "    while True:\n",
        "        # get batch_size random audios with correct resampling and crop\n",
        "        paths = random.sample(filepaths, batch_size)\n",
        "        songs = [decode_audio(path, original_sr, desired_sr, duration)[0]\n",
        "                 for path in paths]\n",
        "\n",
        "        # get indices to crop negative samples\n",
        "        max_start_indices = desired_sr * duration - k * d  # max allowed start index for negative samples\n",
        "        start = tfnp.random.randint(0, max_start_indices, size=(batch_size))  # random indices for negative samples\n",
        "        end = start + k * d\n",
        "\n",
        "        # return batch_size samples at the end\n",
        "        batch = []\n",
        "        for idx in range(batch_size):\n",
        "            # selecte a single postive sample\n",
        "            samples = []  # single postive sample + N-1 negative samples\n",
        "            positive_sample = songs[idx]\n",
        "            positive_sample = tf.reshape(positive_sample, (1, T + k, d, 1))\n",
        "            samples.append(positive_sample)\n",
        "\n",
        "            # get all negative samples from the same batch with random start point\n",
        "            [samples.append(tf.reshape(audio[start[i]:end[i]],\n",
        "                                       (1, k, d, 1))) for i, audio in enumerate(songs) if i != idx]\n",
        "\n",
        "            # concat to one sample with shape (1, T +k*N, d, 1)\n",
        "            batch.append(tf.concat(samples, axis=1))\n",
        "\n",
        "        yield tf.concat(batch, axis=0)  # yield a complete batch from single samples\n",
        "\n",
        "\n",
        "def get_dataset_CPC():\n",
        "    \"\"\"\n",
        "    Uses a global dictionary \"data_generator_arguments\" to create a tf dataset from a generator that outputs batches already.\n",
        "\n",
        "    The data_generator_arguments dictionary has the following arguments:\n",
        "\n",
        "    T:                  Number of time-steps (each being an audio window) used for prediction\n",
        "    k:                  Number of time-steps (each being an audio window) to predict\n",
        "    N:                  Number of negative samples (false/random prediction audio-windows)\n",
        "    original_sr:        Sampling rate of the audio files\n",
        "    desired_sr:         Sampling rate used for resampling the audio files (can reduce computational load but cuts high frequencies)\n",
        "    full_duration:      Length of audio files (shorter files get padded, longer files get cropped)\n",
        "    filepaths:          List of filepaths to wav files.\n",
        "    \"\"\"\n",
        "\n",
        "    T = data_generator_arguments[\"T\"]\n",
        "    k = data_generator_arguments[\"k\"]\n",
        "    N = data_generator_arguments[\"N\"]\n",
        "    sampling_rate = data_generator_arguments[\"desired_sr\"]\n",
        "    batch_size = N\n",
        "    duration = data_generator_arguments[\"full_duration\"]\n",
        "    sr = data_generator_arguments[\"desired_sr\"]\n",
        "\n",
        "    # output shape of generator given the arguments\n",
        "    data_shape = (batch_size, T+k*N, int((duration*sr)/(T+k)), 1)\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_generator(\n",
        "        generator = gen_batch_CPC,\n",
        "        output_signature = tf.TensorSpec(data_shape, \n",
        "                                        dtype=tf.dtypes.float32,\n",
        "                                        name=None)\n",
        "                                        )\n",
        "    \n",
        "    train_ds = train_ds.prefetch(tf.data.AUTOTUNE).cache()\n",
        "\n",
        "    return train_ds\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8MczzCbkBkX"
      },
      "source": [
        "### GTZAN data\n",
        "def create_tfds(inputs, targets, batch_size=None, buffer_size=None, prefetch_factor=None):\n",
        "    '''\n",
        "    Create an input pipeline from tf.dataset.\n",
        "    Adjusted to only take input as there are no labels for autoencoders.\n",
        "    '''\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
        "    if not buffer_size is None:\n",
        "        dataset = dataset.shuffle(buffer_size)\n",
        "    if not batch_size is None:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "    if not prefetch_factor is None:\n",
        "        dataset = dataset.prefetch(prefetch_factor)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def ds_classifier(split_rate, batch_size, path):\n",
        "    # GTZAN default features\n",
        "    pd_gtzan = pd.read_csv(path)\n",
        "    gtzan_feat = pd_gtzan.loc[:, 'chroma_stft_mean':'mfcc20_var'].to_numpy()\n",
        "    gtzan_labels = np.expand_dims(pd_gtzan.loc[:, 'label'].to_numpy(), axis=1)\n",
        "\n",
        "    # Standardize the data to make each feature scale invariant\n",
        "    gtzan_mean = np.expand_dims(np.mean(gtzan_feat, axis=0), axis=0)\n",
        "    gtzan_std = np.expand_dims(np.std(gtzan_feat, axis=0), axis=0)\n",
        "    gtzan_feat = (gtzan_feat - gtzan_mean) / gtzan_std\n",
        "\n",
        "    # one_hot encode labels\n",
        "    encoder = OneHotEncoder()\n",
        "    gtzan_targets = encoder.fit_transform(gtzan_labels).toarray()\n",
        "\n",
        "    # return a tuple of train and test dataset\n",
        "    split_int = int(split_rate * gtzan_feat.shape[0])\n",
        "    return (create_tfds(gtzan_feat[:split_int], gtzan_targets[:split_int], batch_size=batch_size,\n",
        "                        buffer_size=1024, prefetch_factor=tf.data.experimental.AUTOTUNE),\n",
        "            create_tfds(gtzan_feat[split_int:], gtzan_targets[split_int:], batch_size=batch_size,\n",
        "                        buffer_size=1024, prefetch_factor=tf.data.experimental.AUTOTUNE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKu5gcWllgpn"
      },
      "source": [
        "# encoder_model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLx_9dwqlmCX"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class Encoder (tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Encodes an input 1D sequence into an audio window embedding.\n",
        "\n",
        "    z_dim: size of embedding\n",
        "\n",
        "    stride_sizes: list of stride arguments for Conv1D layers\n",
        "    kernel_sizes: list of kernel size arguments for Conv1D layers\n",
        "    n_filters:    list of filter number arguments for Conv1D layers\n",
        "    activation:   activation function used in Conv1D layers and for output Dense layer. (e.g. \"relu\" or tf.nn.relu)\n",
        "    '''\n",
        "\n",
        "    def __init__ (self, z_dim, stride_sizes, kernel_sizes, n_filters, activation):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        s = stride_sizes\n",
        "        k = kernel_sizes\n",
        "        f = n_filters       \n",
        "\n",
        "        self.enc_layers = []\n",
        "\n",
        "        for l in range(len(f)):\n",
        "            self.enc_layers.append(tf.keras.layers.Conv1D(f[l],k[l],s[l]))\n",
        "            self.enc_layers.append(tf.keras.layers.BatchNormalization())\n",
        "            self.enc_layers.append(tf.keras.layers.Activation(activation))\n",
        "        self.enc_layers.append(tf.keras.layers.Flatten())\n",
        "        self.enc_layers.append(tf.keras.layers.Dropout(0.1))\n",
        "        self.enc_layers.append(tf.keras.layers.Dense(z_dim))\n",
        "        self.enc_layers.append(tf.keras.layers.Activation(activation))\n",
        "        \n",
        "    def call (self, x, training):\n",
        "        # input dim: [batch, T+K*N, window_size, 1]\n",
        "\n",
        "        for l in self.enc_layers:\n",
        "            try:\n",
        "                x = l(x, training)\n",
        "            except:\n",
        "                x = l(x)\n",
        "\n",
        "        # ouput dim:[batch, T+K*N, z]\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-P_F_6lmBMm"
      },
      "source": [
        "# autoreg_model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pfjaMe8mEaE"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# TODO: use transformer\n",
        "class Autoregressive (tf.keras.layers.Layer):\n",
        "    '''\n",
        "    GRU RNN that takes a sequence of audio window embeddings and combines them into a context embedding.\n",
        "    c_dim: length of context embedding vector\n",
        "    '''\n",
        "\n",
        "    def __init__ (self, c_dim):\n",
        "        super(Autoregressive, self).__init__()\n",
        "        self.gru = tf.keras.layers.GRU(c_dim, name='ar_context',)\n",
        "        \n",
        "\n",
        "    def call (self, z_sequence):\n",
        "                                    # input dim: [batch, T, z]\n",
        "        return self.gru(z_sequence) # output dim:[batch, c]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFRPooRSEoud"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)  # (1, position, d_model)\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable\n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "                                tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "                                tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "                                ])  \n",
        " \n",
        "\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        '''\n",
        "        d_model: z_dim\n",
        "        num_heads: sets of qkv\n",
        "        dff: num_units for first ffn layer (d_model is for second layer)\n",
        "        maximum_position_encoding: T\n",
        "        '''\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # TODO: embedding layer isn't needed as the input is already embedded\n",
        "        # self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                                self.d_model)\n",
        "\n",
        "        # TODO: would it possible to have different zdim and cdim?\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                        for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]  # T\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        # x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        # TODO: we need (batch_size, d_model)\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVuUpUpgmH_y"
      },
      "source": [
        "# cpc_model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMx95EqumNh7"
      },
      "source": [
        "import tensorflow as tf\n",
        "# from autoreg_model import Autoregressive\n",
        "# from autoencoder_model import Encoder\n",
        "\n",
        "\n",
        "def compute_f (z, z_pred):\n",
        "    '''\n",
        "    Compute f-scores following eq(3) in the paper to be batch (K x N) matrices.\n",
        "    Computes similarity (f-)scores as the exp of the dot product of two embeddings. \n",
        "    First column of the returned f-score matrix is the postive sample.\n",
        "\n",
        "    Compute the similarity scores between (k x N) different z_(t+k) and\n",
        "    k different transformed c_t. First column is the positive sample.\n",
        "    Currently use a log-bilinear model.\n",
        "    z: tf.array, k x N different encoded future time segments\n",
        "    z_pred: tf.array, k different predictions of future time segments\n",
        "    return: tf.array, score matrix for all combinations of future time segments\n",
        "    and N different samples.\n",
        "    '''                                                                                         \n",
        "                                                                               # z_pred input dim: [batch, K, z]\n",
        "                                                                               # z input dim:      [batch, K, N, z]\n",
        "    z = tf.expand_dims(z, axis=-2)                                                  # -> [batch, K, N, 1, z]\n",
        "    \n",
        "                                                                        \n",
        "    pred = tf.repeat(z_pred, repeats=z.shape[2], axis=-2)                           # -> [batch, K*N, z]\n",
        "    pred = tf.reshape(pred, shape=[z.shape[0],z.shape[1],z.shape[2],z.shape[-1]])   # -> [batch, K, N, z]\n",
        "    pred = tf.expand_dims(pred, axis=-1)                                            # -> [batch, K, N, z, 1]\n",
        "\n",
        "    dot_prod = tf.linalg.matmul(z, pred)                                            # -> [batch, K, N, 1, 1]\n",
        "    # TODO: normalize to avoid inf loss, use cosine sim.?\n",
        "    dot_prod = dot_prod/ (tf.norm(z)*tf.norm(pred))\n",
        "    dot_prod = tf.squeeze(dot_prod, axis=[-2,-1])                                   # -> [batch, K, N]\n",
        "    f_mat = tf.exp(dot_prod)\n",
        "\n",
        "                                                                                    # output dim: [batch, K, N]\n",
        "    return f_mat \n",
        "\n",
        "\n",
        "# TODO: use GRU?\n",
        "class Predict_z (tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Layer that uses the context embedding c_t to predict K (future) embeddings\n",
        "    Transform c_t for each timestep t+k to predict future timesteps.\n",
        "    Currently linear (W_k) for all future timesteps.\n",
        "    z_dim: int, dimension of encoded future time segments\n",
        "    K: int, num. future time segments to predict\n",
        "    '''\n",
        "\n",
        "    def __init__ (self, z_dim, K):\n",
        "        super(Predict_z, self).__init__()\n",
        "        \n",
        "        # input_dim: [batch, c_dim]\n",
        "        self.transform_layers = []\n",
        "\n",
        "        # one linear layer for each future time-step\n",
        "        for k in tf.range(K):  \n",
        "            self.transform_layers.append(tf.keras.layers.Dense(z_dim))\n",
        "\n",
        "    def call(self, c_t):\n",
        "        \n",
        "        z_pred = tf.TensorArray(tf.float32, size=len(self.transform_layers))\n",
        "\n",
        "        for l,layer in enumerate(self.transform_layers):\n",
        "            # apply linear projection layer for each k\n",
        "            z_pred = z_pred.write(l, layer(c_t))                        \n",
        "\n",
        "        z_pred_t = z_pred.stack()\n",
        "        # output_dim: [batch, K, z]\n",
        "        return tf.transpose(z_pred_t, perm=[1,0,2])       \n",
        "\n",
        "\n",
        "class CPC(tf.keras.models.Model):\n",
        "    '''\n",
        "    CPC model with encoder, autoregressive model and prediction models.\n",
        "    num_time_observations: int, num. time segments that is observed\n",
        "    num_time_future: int, num. time segments to be predicted\n",
        "    num_samples: int, num. samples to be used, which is 1 + num_neg_samples\n",
        "    z_dim: int, dimensionality to which inputs are encoded\n",
        "    c_dim: int, dimensionality of the context vector from AR\n",
        "    encoder_args: dict, argument dictionary for Encoder model\n",
        "    return: tf.array, similarity score matrix for (k x N) combinations\n",
        "    '''\n",
        "\n",
        "\n",
        "    # TODO: args, encoder and autoregressive\n",
        "    def __init__(self, num_time_observations, num_time_future, num_samples, z_dim, c_dim, encoder_args):\n",
        "        super(CPC, self).__init__()\n",
        "        self.T = num_time_observations\n",
        "        self.K = num_time_future\n",
        "        self.N = num_samples\n",
        "        self.z = z_dim\n",
        "        self.c = c_dim\n",
        "\n",
        "        self.g_enc = Encoder(**encoder_args)\n",
        "        self.g_ar = Autoregressive(self.c)\n",
        "        self.p_z = Predict_z(z_dim=self.z, K=self.K)\n",
        "\n",
        "    # @tf.function\n",
        "    def get_embedding(self, x):\n",
        "        \"\"\"\n",
        "        Calculate embeddings for full input.\n",
        "        \"\"\"\n",
        "        z_t = tf.keras.layers.TimeDistributed(self.g_enc)(x, training=False)\n",
        "        c_T = self.g_ar(z_t)\n",
        "\n",
        "        return c_T\n",
        "\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        # input dim: [batch, T+K*N, d, 1]\n",
        "        # Embedding\n",
        "        # we need enc first then ar for transformer. Best if we used 'mode' arg.\n",
        "        z_t = tf.keras.layers.TimeDistributed(  # dim 1 is the temporal dim\n",
        "            self.g_enc)(x, training=training)  # [batch, T+K*N, z]\n",
        "\n",
        "        # Split into current observation embeddings and (positive and negative) future embeddings\n",
        "        z_obs = z_t[:, :self.T]  # [batch, T, z]\n",
        "        z_future = z_t[:, self.T:]  # [batch, K*N, z]\n",
        "        z_future = tf.reshape(z_future, [-1, self.K, self.N, self.z])  # [batch, K, N, z]\n",
        "\n",
        "        # Predict embeddings\n",
        "        c_T = self.g_ar(z_obs)  # [batch, c]\n",
        "        z_pred = self.p_z(c_T)  # [batch, K, z]\n",
        "\n",
        "        # Compute f matrices\n",
        "        f_mat = compute_f(z_future, z_pred)  # [batch, K, N]\n",
        "        return f_mat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPILFCYLTfga"
      },
      "source": [
        "class InfoNCE (tf.keras.losses.Loss):\n",
        "    '''\n",
        "    Compute InfoNCE loss given a batch of f matrices with dim (K x N)\n",
        "    '''\n",
        "\n",
        "    # TODO: trick to avoid inf loss\n",
        "    def __init__(self):\n",
        "        self.eps = tf.constant(0.000001)\n",
        "\n",
        "    def __call__(self, f):\n",
        "        f = f + self.eps\n",
        "                                                         # input dim: [batch, K, N]\n",
        "        denominator = tf.reduce_sum(f, axis=2)           # -> [batch, K]\n",
        "        losses = - tf.math.log(f[:,:,0] / denominator)  # first column is the positive k predictions\n",
        "        loss = tf.reduce_mean(losses, axis=None)         # Take mean loss over batch_size and K\n",
        "\n",
        "        if not tf.math.is_finite(loss):\n",
        "            tf.print(\"inf loss\")\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8WRwvAwmY7a"
      },
      "source": [
        "# classifier_model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzSqtT-hmYZD"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Classifier(tf.keras.Model):\n",
        "    def __init__(self, classes, act_fct='relu'):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.hidden_layers = []\n",
        "\n",
        "        units = [256, 128, 64, 32]\n",
        "\n",
        "        for i in range(len(units) - 1):\n",
        "            self.hidden_layers.append(tf.keras.layers.Dense(units=units[i]))\n",
        "            self.hidden_layers.append(tf.keras.layers.BatchNormalization())\n",
        "            self.hidden_layers.append(tf.keras.layers.Activation(act_fct))\n",
        "\n",
        "        self.output_layer = tf.keras.layers.Dense(units=classes,\n",
        "                                                  activation='softmax',\n",
        "                                                  use_bias=False)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        for l in self.hidden_layers:\n",
        "            try:\n",
        "                x = l(x, training)\n",
        "            except:\n",
        "                x = l(x)\n",
        "\n",
        "        return self.output_layer(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j3Smkx6mp7f"
      },
      "source": [
        "# training.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqNK2ss8mrQp"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "@tf.function\n",
        "def train_step_cpc(model, ds, loss_function, optimizer, steps_per_epoch,\n",
        "               train_loss_metric=None):\n",
        "\n",
        "    for batch in ds.take(steps_per_epoch):\n",
        "        # forward pass with GradientTape\n",
        "        with tf.GradientTape() as tape:\n",
        "            prediction = model(batch, training=True)\n",
        "            loss = loss_function(prediction)\n",
        "\n",
        "        # backward pass via GradienTape (auto-gradient calc)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        # update metrics\n",
        "        if train_loss_metric is not None:\n",
        "            train_loss_metric.update_state(loss)\n",
        "            \n",
        "\n",
        "@tf.function\n",
        "def train_step_classify(model, ds, loss_function, optimizer, train_loss_metric, train_acc_metric):\n",
        "    for x, target in ds:\n",
        "        with tf.GradientTape() as tape:\n",
        "            prediction = model(x)\n",
        "            loss = loss_function(target, prediction)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        # update metrics\n",
        "        train_loss_metric.update_state(loss)\n",
        "        train_acc_metric.update_state(target, prediction)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def test_step(model, ds, loss_function, loss_metric, acc_metric):\n",
        "    for x, target in ds:\n",
        "        prediction = model(x)\n",
        "        loss = loss_function(target, prediction)\n",
        "\n",
        "        # update metrics\n",
        "        loss_metric.update_state(loss)\n",
        "        acc_metric.update_state(target, prediction)\n",
        "\n",
        "\n",
        "def eval_metric(metric, val_list):\n",
        "    result = metric.result()\n",
        "    metric.reset_states()\n",
        "    val_list.append(result)\n",
        "    return result\n",
        "\n",
        "\n",
        "# TODO: eventually do training in a main loop and not with a single func\n",
        "def main_train_eval_loop(mode, epochs, model, ds_train, ds_test, loss_function, optimizer, steps_per_epoch,\n",
        "                         train_loss_metric=None, train_acc_metric=None, test_loss_metric=None, test_acc_metric=None,\n",
        "                         PATH=False):\n",
        "    '''\n",
        "    Call appropriate train and test steps depending on the mode.\n",
        "    :param mode: str, either 'CPC' or 'classifier'. 'CPC' only calls train, 'classifier' calls train and test steps.\n",
        "    :param epochs: int, number of epochs to train\n",
        "    :param model: tf.keras.model, model to train\n",
        "    :param ds_train: tf.keras.data.dataset, dataset to train on\n",
        "    :param ds_test: tf.keras.data.dataset, dataset to test on\n",
        "    :param loss_function: tf.keras.loss, loss function to use\n",
        "    :param optimizer: tf.keras.optimizer, optimizer to use\n",
        "    :param steps_per_epoch: int, number of batch to feed per epoch\n",
        "    :param train_loss_metric: tf.keras.metric\n",
        "    :param train_acc_metric:\n",
        "    :param test_loss_metric: tf.keras.metric\n",
        "    :param test_acc_metric:\n",
        "    :param PATH: str, path to save to trained model weights\n",
        "    :return: list of list, 4 metrics for all epochs\n",
        "    '''\n",
        "\n",
        "    # arrays to save results for all epochs\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for e in range(epochs):\n",
        "        # Train\n",
        "        if mode == 'CPC':\n",
        "            train_step_cpc(model, ds_train, loss_function, optimizer, steps_per_epoch, train_loss_metric)\n",
        "        elif mode == 'classifier':\n",
        "            train_step_classify(model, ds_train, loss_function, optimizer, train_loss_metric, train_acc_metric)\n",
        "        if train_loss_metric is not None:\n",
        "            eval_metric(train_loss_metric, train_losses)\n",
        "        if train_acc_metric is not None:\n",
        "            eval_metric(train_acc_metric, train_accuracies)\n",
        "\n",
        "        # Evaluate\n",
        "        if mode == 'classifier':\n",
        "            test_step(model, ds_test, loss_function, test_loss_metric, test_acc_metric)\n",
        "        if test_loss_metric is not None:\n",
        "            eval_metric(test_loss_metric, test_losses)\n",
        "        if test_acc_metric is not None:\n",
        "            eval_metric(test_acc_metric, test_accuracies)\n",
        "\n",
        "    # Save model weights\n",
        "    if PATH:\n",
        "        from datetime import datetime\n",
        "        now = datetime.now()\n",
        "        save_to = PATH + str(now)[:-10] + \".h5\"\n",
        "        model.save_weights(save_to, overwrite=False)\n",
        "\n",
        "    return train_losses, train_accuracies, test_losses, test_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHSHQf3ylNpp"
      },
      "source": [
        "# Main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WINQkBAxgtAc"
      },
      "source": [
        "### (A) Train the CPC model ###\n",
        "T = data_generator_arguments[\"T\"]\n",
        "k = data_generator_arguments[\"k\"]\n",
        "N = data_generator_arguments[\"N\"]\n",
        "batch_size = N\n",
        "\n",
        "# Generate a dataset\n",
        "ds_train_cpc = get_dataset_CPC()\n",
        "\n",
        "\n",
        "# Define 3 design components\n",
        "# Model\n",
        "cpc = CPC(T, k, N, z_dim, c_dim, encoder_args)\n",
        "# load trained model\n",
        "if load_path:\n",
        "    cpc.load_weights(load_path)\n",
        "# Loss\n",
        "infonce = InfoNCE()\n",
        "train_loss_metric_cpc = tf.keras.metrics.Mean('train_loss_CPC')\n",
        "# Optimizer\n",
        "adam = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "\n",
        "# Training\n",
        "res_train_loss_cpc, *_ = main_train_eval_loop('CPC', epochs, cpc, ds_train_cpc, None, infonce, adam, steps_per_epoch=batch_size,\n",
        "                                          train_loss_metric=train_loss_metric_cpc, PATH=weight_path)\n",
        "# TODO: visualize loss-epoch using tensorboard\n",
        "######\n",
        "\n",
        "\n",
        "\n",
        "### (B,C) Get a baseline performance of a classifier using GTZAN pre-found features ###\n",
        "# Generate a dataset\n",
        "gtzan_train, gtzan_test = ds_classifier(split_rate, batch_size, gtzan_feature_path)\n",
        "\n",
        "\n",
        "# Define 3 design components\n",
        "# Model\n",
        "num_classes = list(gtzan_train.take(1).as_numpy_iterator())[0][1].shape[1]\n",
        "classi1 = Classifier(num_classes)\n",
        "# Loss\n",
        "cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "train_acc_metric_classi1 = tf.keras.metrics.CategoricalAccuracy('train_accuracy_classi1')\n",
        "train_loss_metric_classi1 = tf.keras.metrics.Mean('train_loss_classi1')\n",
        "test_acc_metric_classi1 = tf.keras.metrics.CategoricalAccuracy('test_accuracy_classi1')\n",
        "test_loss_metric_classi1 = tf.keras.metrics.Mean('test_loss_classi1')\n",
        "\n",
        " \n",
        "# Training and testing\n",
        "res_train_loss_c1, res_train_acc_c1, res_test_loss_c1, res_test_acc_c1 = main_train_eval_loop(\n",
        "    'classifier', epochs, classi1, gtzan_train, gtzan_test, cce, adam, steps_per_epoch=8,\n",
        "    train_loss_metric=train_loss_metric_classi1, train_acc_metric=train_acc_metric_classi1,\n",
        "     test_loss_metric=test_loss_metric_classi1, test_acc_metric=test_acc_metric_classi1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717,
          "referenced_widgets": [
            "b98a69c855234882a65b8a4150f348b5",
            "728500ddf8bb416d99176e34492b4b5a",
            "fe8a17bf244846358ce6fe5f92a76f12",
            "6043905dc16f43ce97f06ceabfaa2c00",
            "6d1a31ff352840d79655cb6c4e0b615e",
            "9aeef0ebc3de4a8286283617764d77f7",
            "cc5855476a344208a3b3d2b898b962b5",
            "7aa890101f0a4272ab7c3ee9a02b7676",
            "6f65a390c9b44157bf334fa5307d6184",
            "2760ec8674ac4265870c8b63e810b209",
            "e49d5aad4d1c431692e26d35ef0bc6c7",
            "52096dc5e8bb406f8dd65e2c50a6c5e4",
            "09027621bb7b4994bb286fca52ec8e70",
            "30edf5b6354447869292292523a53c35",
            "3a77a0ef98bb4d44a6580a220f64c5be",
            "b90b82f066904074a47aad1a244d35be",
            "c9fc4aa180784377ad464fd807f6dcd6",
            "be60987cbab04b019175dfacdd246c45",
            "41de04ac05884c29a73b9e482080144b",
            "6faa844540d143a6a1a89dec4e09b36a",
            "186df641e73c4c59b4501fccdf014d80",
            "e80dad1bb9a147a494e5150ceaa2e4ab",
            "00f4aa9f4142460f8f8defafe8bb3459",
            "c71f6982a6974d88aad8d9107bb9650f",
            "e6a7aa406979447087b4efd7204e8e6d",
            "493f6a67ea3c4b94ba570c820b28e9f1",
            "63aaeda1837f4b72af121327c312f92f",
            "aad149381ea24039b3de0c39afc7c0b7",
            "c1853f4b52684d3cab84a886738c54f8",
            "6b3461e6a02f4e31b599b7d822d27eca",
            "4367869afb1942af9be5197947e0d75f",
            "e9d049a635bd42c094b892ff0dcc6467",
            "be778ca54a904522baa4e0e6b5007d30",
            "27c1a4ecb55743b69785cc64870cc5fe",
            "ee35d855f0ea472bb2b594c21f6841b1",
            "d603ba16b640485d97581c4f2cb5bd09",
            "4d50d6dc7ab6475e90747cc0712b3e6d",
            "2134888bda224be69616b7271764922e",
            "0eaf2117ebce46e8aca5f415d223d028",
            "261f5dec0ab54107bc56d25ee5b8dbf1"
          ]
        },
        "id": "fLQi0w4mpXwV",
        "outputId": "20e4ceef-8ac9-4dac-e1f1-2bbc3199e731"
      },
      "source": [
        "### (D,E) Classify with learned features from CPC model\n",
        "# Get features using trained CPC\n",
        "import tensorflow_datasets as tfds\n",
        "gtzan_ds = tfds.load('gtzan', split='train', as_supervised=True).shuffle(1024).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "gtzan_ds_train = gtzan_ds.take(int(1000*split_rate))\n",
        "gtzan_ds_test = gtzan_ds.skip(int(1000*split_rate))\n",
        "\n",
        "\n",
        "def get_embedded_ds(ds, model):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for audio, label in ds:\n",
        "        audio = audio[:(T+k)*d]  # if GTZAN happends to be longer\n",
        "        audio = tf.reshape(audio, (1, T+k, d, 1))  # \n",
        "        print(audio.shape)\n",
        "        # TODO: Value for attr 'T' of int64 is not in the list of allowed values:\n",
        "        features.append(model.get_embedding(audio))\n",
        "        labels.append(label)\n",
        "    print(\"features, labels\")\n",
        "    print(np.array(features).shape, np.array(labels).shape)\n",
        "    return tf.data.Dataset.from_tensor_slices((features,labels))\n",
        "\n",
        "\n",
        "cpc_train_features = get_embedded_ds(gtzan_ds_train, cpc).batch(batch_size)\n",
        "cpc_test_features = get_embedded_ds(gtzan_ds_test, cpc).batch(batch_size)\n",
        "\n",
        "# Design components\n",
        "# model\n",
        "classi2 = Classifier(num_classes)\n",
        "# metrics\n",
        "train_acc_metric_classi2 = tf.keras.metrics.CategoricalAccuracy('train_accuracy_classi2')\n",
        "train_loss_metric_classi2 = tf.keras.metrics.Mean('train_loss_classi2')\n",
        "test_acc_metric_classi2 = tf.keras.metrics.CategoricalAccuracy('test_accuracy_classi2')\n",
        "test_loss_metric_classi2 = tf.keras.metrics.Mean('test_loss_classi2')\n",
        "\n",
        "# Training and testing\n",
        "res_train_loss_c2, res_train_acc_c2, res_test_loss_c2, res_test_acc_c2 = main_train_eval_loop(\n",
        "    'classifier', epochs, classi2, cpc_train_features, cpc_test_features, cce, adam, steps_per_epoch=8,\n",
        "    train_loss_metric=train_loss_metric_classi2, train_acc_metric=train_acc_metric_classi2,\n",
        "     test_loss_metric=test_loss_metric_classi2, test_acc_metric=test_acc_metric_classi2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset gtzan/1.0.0 (download: 1.14 GiB, generated: 3.71 GiB, total: 4.85 GiB) to /root/tensorflow_datasets/gtzan/1.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b98a69c855234882a65b8a4150f348b5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progreâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f65a390c9b44157bf334fa5307d6184",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressStyâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9fc4aa180784377ad464fd807f6dcd6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, stylâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6a7aa406979447087b4efd7204e8e6d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rShuffling and writing examples to /root/tensorflow_datasets/gtzan/1.0.0.incomplete5BWH2D/gtzan-train.tfrecord\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be778ca54a904522baa4e0e6b5007d30",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset gtzan downloaded and prepared to /root/tensorflow_datasets/gtzan/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-b40744036fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mcpc_train_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedded_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgtzan_ds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mcpc_test_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedded_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgtzan_ds_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-b40744036fa8>\u001b[0m in \u001b[0;36mget_embedded_ds\u001b[0;34m(ds, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# if GTZAN happends to be longer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnsVDAcIzX7K"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHLrB4t5u5lM"
      },
      "source": [
        "for i,l in gtzan_ds.take(1):\n",
        "    print(i,l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi032uXWxNAF"
      },
      "source": [
        "# TODO: automatic result and figure saving\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(ncols=2)\n",
        "\n",
        "ax[0].plot(np.array(res_train_loss_c1), color='r')\n",
        "ax[0].plot(np.array(res_test_loss_c1), color='b')\n",
        "ax[0].set(title='cce loss')\n",
        "ax[1].plot(np.array(res_train_acc_c1), color='r')\n",
        "ax[1].plot(np.array(res_test_acc_c1), color='b')\n",
        "ax[1].set(title='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
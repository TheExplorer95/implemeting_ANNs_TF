{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_strings(ds, sentence_wise=True):\n",
    "    # make numpy string array from tfds    \n",
    "    tfds_to_numpy = lambda x: next(iter(x))['text'].numpy()\n",
    "    ds = tfds_to_numpy(ds).decode()                             \n",
    "    \n",
    "    # make list of just words\n",
    "    ds_words = ds.lower().replace('\\n', ' ').translate({ord(\"'\"): None})\n",
    "    exclude = string.punctuation.translate({ord(\"'\"): None})\n",
    "    table = ds_words.maketrans(exclude, ' '*len(exclude))                   \n",
    "    ds_words = np.array(ds_words.translate(table).split())\n",
    "    \n",
    "    # creates two lookup tables, val->id and id->val\n",
    "    dict_to_id = {val: i for i, val in enumerate(sorted(set(ds_words)))}        \n",
    "    dict_to_val = {id_: val for val, id_ in dict_to_id.items()}\n",
    "    vocab_size = len(ds_words)\n",
    "    \n",
    "    # define occurances of each token\n",
    "    word_freq = [np.count_nonzero(ds_words==val) for _, val in dict_to_val.items()]\n",
    "\n",
    "    # create a list of words split into sentences\n",
    "    if sentence_wise: \n",
    "        ds = ds.lower().replace('\\n', ' ').translate({ord(\"'\"): None})\n",
    "        exclude = string.punctuation.translate({ord(\"'\"): None, ord('.'): None})\n",
    "        table = ds.maketrans(exclude, ' '*len(exclude))\n",
    "        ds = ' '.join(ds.translate(table).split()).split('.')\n",
    "        ds = [sentence.translate({ord(\".\"): None}).split() for sentence in ds]        \n",
    "        \n",
    "        ds = [[dict_to_id[word] for word in sentence] for sentence in ds]\n",
    "        \n",
    "    # use list of words\n",
    "    else:\n",
    "        ds = [dict_to_id[word] for words in ds_words]\n",
    "    \n",
    "    return ds, dict_to_id, dict_to_val, word_freq, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tf_dataset(ds, word_to_id, vocab_size, threads=16, batch_size=32):\n",
    "    # no need to shuffle, as dataset is shuffled within generator\n",
    "    \n",
    "    ds = ds.map(lambda x, y: (tf.one_hot(x, depth=vocab_size), \n",
    "                              tf.one_hot(y, depth=vocab_size)), \n",
    "                num_parallel_calls=threads)\n",
    "    \n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3805, 1748, 830, 11048, 7595, 407, 4159, 4686, 6120, 9224], [279, 9224, 9224], [3805, 1748, 11456, 480, 279, 8183, 7913, 10151, 2757, 9960, 10151, 3612, 279, 8183], [8183], [3805, 1748, 3805, 11456, 5520, 1423, 6046, 5321, 1682, 3319, 10151, 9972, 7132]] 183574\n"
     ]
    }
   ],
   "source": [
    "train_ds = tfds.load(name='tiny_shakespeare',\n",
    "                    shuffle_files=False, \n",
    "                    split='train')\n",
    "\n",
    "train_ds, train_to_id, train_to_val, word_freq, vocab_size = preprocess_strings(train_ds)\n",
    "\n",
    "print(train_ds[0:5], vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11048\n",
      "11048\n",
      "11048\n",
      "11048\n",
      "10151\n",
      "10151\n",
      "10151\n",
      "10151\n",
      "6007\n",
      "6007\n",
      "6007\n",
      "6007\n",
      "2978\n"
     ]
    }
   ],
   "source": [
    "ds = train_ds\n",
    "\n",
    "def gen_word_embeddings():    \n",
    "    while True:\n",
    "        np.random.shuffle(ds)      \n",
    "        \n",
    "        # for each sentence generate one target and make input, target pairs from leftover words within sentence\n",
    "        for sentence in ds:\n",
    "            if len(sentence) == 0: continue\n",
    "            \n",
    "            word_id = np.random.randint(0, len(sentence))\n",
    "            word = sentence[word_id]\n",
    "            \n",
    "            context_window = sentence[word_id-2:word_id] + sentence[word_id+1:word_id+3]\n",
    "            np.random.shuffle(context_window)\n",
    "            \n",
    "            for target in context_window:\n",
    "                yield word, target\n",
    "                \n",
    "gen = gen_word_embeddings()\n",
    "for i in range(13):\n",
    "    print(next(gen)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6410, shape=(), dtype=int64)\n",
      "6410\n",
      "tf.Tensor(6410, shape=(), dtype=int64)\n",
      "6410\n",
      "tf.Tensor(6410, shape=(), dtype=int64)\n",
      "6410\n",
      "tf.Tensor(6410, shape=(), dtype=int64)\n",
      "6410\n",
      "tf.Tensor(4107, shape=(), dtype=int64)\n",
      "4107\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(32, 183574), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(32, 183574), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_generator(gen_word_embeddings,\n",
    "                               output_signature=(tf.TensorSpec(shape=(), dtype=tf.int64),\n",
    "                                                 tf.TensorSpec(shape=(), dtype=tf.int64)))\n",
    "\n",
    "for i, x in train_ds.take(5):\n",
    "    print(i)\n",
    "    print(i.numpy())\n",
    "    \n",
    "train_ds = preprocess_tf_dataset(train_ds, train_to_id, vocab_size)\n",
    "\n",
    "for x, t in train_ds.take(1):\n",
    "    print(x)\n",
    "    print(t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(32, 183574), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(32, 183574), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for x, t in train_ds.take(1):\n",
    "    print(x)\n",
    "    print(t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.act = tf.keras.layers.Softmax()\n",
    "        \n",
    "    def build(self, shape):\n",
    "        self.w_in_embedding = self.add_weight(shape=(self.vocab_size, self.embedding_size), \n",
    "                                         initializer=\"random_normal\", \n",
    "                                         trainable=True)\n",
    "\n",
    "        # no bias as recommended\n",
    "#         self.w_in_embedding_bias = self.add_weight(shape=(self.embedding_size), \n",
    "#                                          initializer=\"zeros\", \n",
    "#                                          trainable=True)\n",
    "        \n",
    "        \n",
    "        self.w_out_embedding = self.add_weight(shape=(self.embedding_size, self.vocab_size), \n",
    "                                   initializer=\"random_normal\", \n",
    "                                   trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        # standart way\n",
    "        in_embedding = tf.matmul(x, self.w_in_embedding)\n",
    "        score_vec = tf.matmul(in_embedding, self.w_out_embedding)\n",
    "        context_word = self.act(score_vec)\n",
    "\n",
    "        # recommended with lookup\n",
    "#         embed = tf.nn.embedding_lookup(self.w_in_embedding, tf.argmax(x, axis=-1))\n",
    "        \n",
    "#         loss = tf.nn.nce_loss(weights=self.w_in_embedding,             # [vocab_size, embed_size]\n",
    "#                              biases=self.w_in_embedding_bias,          # [vocab_size]\n",
    "#                              labels=target,                            # [bs, 1]\n",
    "#                              inputs=embed,                                 # [bs, embed_size]\n",
    "#                              num_sampled=20,\n",
    "#                              num_classes=self.vocab_size)\n",
    "       \n",
    "        return context_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    \"\"\"\n",
    "    A small class for making timings.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Start a new timer\n",
    "        \"\"\"\n",
    "        if self._start_time is not None:\n",
    "            raise TimerError(f\"Timer is running. Use .stop() to stop it\")\n",
    "\n",
    "        self._start_time = time.perf_counter()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"\n",
    "        Stop the timer, and report the elapsed time\n",
    "        \"\"\"\n",
    "        if self._start_time is None:\n",
    "            print(f\"Timer is not running. Use .start() to start it\")\n",
    "            return 0\n",
    "    \n",
    "        elapsed_time = time.perf_counter() - self._start_time\n",
    "        self._start_time = None\n",
    "        return elapsed_time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SkipGram_train(model, train_ds, loss_function, optimizer, train_loss_metric, train_acc_metric):\n",
    "\n",
    "    for x, target in train_ds.take(100):\n",
    "        # forward pass with GradientTape\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = model(x)\n",
    "            loss = loss_function(target, prediction)\n",
    "\n",
    "        # backward pass via GradienTape (auto-gradient calc)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        train_loss_metric.update_state(loss)\n",
    "        train_acc_metric.update_state(target, prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "beta = 0.95\n",
    "embedding_dim = 64\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "SG_model = SkipGram(vocab_size, embedding_dim)\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,  beta)\n",
    "\n",
    "timer = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare metrics\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy('train_accuracy')\n",
    "# test_acc_metric = tf.keras.metrics.CategoricalAccuracy('test_accuracy')\n",
    "\n",
    "train_loss_metric = tf.keras.metrics.Mean('train_loss')\n",
    "# test_loss_metric = tf.keras.metrics.Mean('test_loss')\n",
    "\n",
    "# Initialize lists for later visualization.\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "# test_losses = []\n",
    "# test_accuracies = []\n",
    "times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH] ____________________0____________________\n",
      "[0] - Finished Epoch in 5.61 seconds - train_loss: 12.1193, train_acc: 0.0000\n",
      "\n",
      "[INFO] - Total time elapsed: 0.0935 min. Total time remaining: 22.3537 min.\n",
      "\n",
      "[EPOCH] ____________________1____________________\n",
      "[1] - Finished Epoch in 5.31 seconds - train_loss: 12.1141, train_acc: 0.0016\n",
      "\n",
      "[EPOCH] ____________________2____________________\n",
      "[2] - Finished Epoch in 5.21 seconds - train_loss: 12.0883, train_acc: 0.0131\n",
      "\n",
      "[EPOCH] ____________________3____________________\n",
      "[3] - Finished Epoch in 5.19 seconds - train_loss: 11.9829, train_acc: 0.0253\n",
      "\n",
      "[INFO] - Total time elapsed: 0.3553 min. Total time remaining: 20.9624 min.\n",
      "\n",
      "[EPOCH] ____________________4____________________\n",
      "[4] - Finished Epoch in 5.24 seconds - train_loss: 11.6916, train_acc: 0.0231\n",
      "\n",
      "[EPOCH] ____________________5____________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-afa910ff7c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mSkipGram_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSG_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Evaluating training metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1f4371b91cf2>\u001b[0m in \u001b[0;36mSkipGram_train\u001b[0;34m(model, train_ds, loss_function, optimizer, train_loss_metric, train_acc_metric)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# backward pass via GradienTape (auto-gradient calc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# update metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_2.4/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    623\u001b[0m             \"ParameterServerStrategy and CentralStorageStrategy\")\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m       \u001b[0mapply_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mexperimental_aggregate_gradients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_unaggregated_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_2.4/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_prepare\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    878\u001b[0m       \u001b[0mapply_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_2.4/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/adam.py\u001b[0m in \u001b[0;36m_prepare_local\u001b[0;34m(self, var_device, var_dtype, apply_state)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mbeta_1_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_hyper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'beta_1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mbeta_2_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_hyper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'beta_2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mbeta_1_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_1_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mbeta_2_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_2_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     lr = (apply_state[(var_device, var_dtype)]['lr_t'] *\n",
      "\u001b[0;32m~/miniconda3/envs/tf_2.4/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_2.4/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mpow\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    668\u001b[0m   \"\"\"\n\u001b[1;32m    669\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Pow\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_2.4/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_pow\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6549\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6550\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 6551\u001b[0;31m         _ctx, \"Pow\", name, x, y)\n\u001b[0m\u001b[1;32m   6552\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6553\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 240\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\n[EPOCH] ____________________{epoch}____________________')\n",
    "    \n",
    "    # training step with metrics update--------------------------------------------------------\n",
    "    timer.start()\n",
    "\n",
    "    SkipGram_train(SG_model, train_ds, loss_function, optimizer, train_loss_metric, train_acc_metric)\n",
    "\n",
    "    # Evaluating training metrics\n",
    "    train_loss = train_loss_metric.result()\n",
    "    train_acc = train_acc_metric.result()\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    elapsed_time = timer.stop()\n",
    "    times.append(elapsed_time)\n",
    "    \n",
    "    print(f'[{epoch}] - Finished Epoch in {elapsed_time:0.2f} seconds - train_loss: {train_loss:0.4f}, train_acc: {train_acc:0.4f}')\n",
    "    \n",
    "    # evaluation step with metrics update--------------------------------------------------------\n",
    "#     timer.start()\n",
    "\n",
    "#     eval_step(model, test_ds, loss_function, \n",
    "#               loss_metric=test_loss_metric, \n",
    "#               acc_metric=test_acc_metric)\n",
    "\n",
    "#     # Evaluating validation metrics\n",
    "#     test_loss = test_loss_metric.result()\n",
    "#     test_acc = test_acc_metric.result()\n",
    "#     test_losses.append(test_loss)\n",
    "#     test_accuracies.append(test_acc)\n",
    "    \n",
    "#     print(f'\\n[{epoch}] - Finished evaluation - test_loss: {test_loss:0.4f}, test_accuracy: {test_acc:0.4f}')\n",
    "    \n",
    "    # Resetting train and validation metrics-----------------------------------------------------\n",
    "    train_acc_metric.reset_states()\n",
    "#     test_acc_metric.reset_states()\n",
    "    train_loss_metric.reset_states()\n",
    "#     test_loss_metric.reset_states()\n",
    "    \n",
    "#     elapsed_time = timer.stop()\n",
    "#     times.append(elapsed_time)\n",
    "  \n",
    "    if epoch%3 == 0:\n",
    "        print(f'\\n[INFO] - Total time elapsed: {np.sum(times)/60:0.4f} min. Total time remaining: {(np.sum(times)/(epoch+1))*(epochs-epoch-1)/60:0.4f} min.')\n",
    "\n",
    "print(f'[INFO] - Total run time: {np.sum(times)/60:0.4f} min.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
